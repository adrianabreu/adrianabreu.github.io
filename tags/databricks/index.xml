<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DataBricks on Adrián Abreu</title><link>https://adrianabreu.com/tags/databricks/</link><description>Recent content in DataBricks on Adrián Abreu</description><generator>Hugo</generator><language>es-ES</language><copyright>2017-2024 Adrián Abreu powered by Hugo and Kiss Theme</copyright><lastBuildDate>Fri, 26 Jan 2024 09:06:32 +0000</lastBuildDate><atom:link href="https://adrianabreu.com/tags/databricks/index.xml" rel="self" type="application/rss+xml"/><item><title>Querying the databricks api</title><link>https://adrianabreu.com/blog/2024-01-26-querying-the-databricks-api/</link><pubDate>Fri, 26 Jan 2024 09:06:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2024-01-26-querying-the-databricks-api/</guid><description>&lt;p&gt;Exploring databricks SQL usage&lt;/p&gt;
&lt;p&gt;At my company, we adopted databricks SQL for most of our users. Some users have developed applications that use the JDBC connector, some users have built their dashboards, and some users write plain ad-hoc queries.&lt;/p&gt;
&lt;p&gt;We wanted to know what they queried, so we tried to use Unity Catalog&amp;rsquo;s insights, but it wasn&amp;rsquo;t enough for our case. We work with IOT and we are interested in what filters they apply within our tables.&lt;/p&gt;</description></item><item><title>Tweaking Spark Kafka</title><link>https://adrianabreu.com/blog/2023-10-27-tweaking-spark-kafka/</link><pubDate>Fri, 27 Oct 2023 12:06:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2023-10-27-tweaking-spark-kafka/</guid><description>&lt;p&gt;Well, I&amp;rsquo;m facing a huge interesting case. I&amp;rsquo;m working at Wallbox where we need to deal with billions of rows every day. Now we need to use Spark for some Kafka filtering and publish the results into different topics according to some rules.&lt;/p&gt;
&lt;p&gt;I won&amp;rsquo;t dig deep into the logic except for performance-related stuff, let&amp;rsquo;s try to increase the processing speed.&lt;/p&gt;
&lt;p&gt;When reading from Kafka you usually get 1 task per partition, so if you have 6 partitions and 48 cores you are not using 87.5 percent of your cluster. That could be adjusted with the following property &lt;code&gt;**minPartitions&lt;/code&gt;.**&lt;/p&gt;</description></item><item><title>Repairing metadata unity catalog</title><link>https://adrianabreu.com/blog/2023-10-02-repairing-metadata-unity-catalog/</link><pubDate>Mon, 02 Oct 2023 13:25:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2023-10-02-repairing-metadata-unity-catalog/</guid><description>&lt;p&gt;I&amp;rsquo;ve been subscribed to &lt;a href="https://www.dataengineeringweekly.com/p/data-engineering-weekly-148"&gt;https://www.dataengineeringweekly.com/p/data-engineering-weekly-148&lt;/a&gt; for years. This last number included several on-call posts on Medium. I found these quite useful.&lt;/p&gt;
&lt;p&gt;Today, I got an alert from Metaplane that a cost monitor dashboard was out of date. I checked the processes, and everything was fine. I ran a query to check the freshness of the data and it was ok too.&lt;/p&gt;
&lt;p&gt;Metaplane checks our delta table freshness by querying the table information available in the Unity Catalog. For some unknown reason that metadata didn&amp;rsquo;t receive any update. I ran an optimization operation (the table tiny) and the metadata didn&amp;rsquo;t update either.&lt;/p&gt;</description></item><item><title>Adding extra params on DatabricksRunNowOperator</title><link>https://adrianabreu.com/blog/2023-07-28-extra_params_databricksrunnow/</link><pubDate>Fri, 28 Jul 2023 16:00:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2023-07-28-extra_params_databricksrunnow/</guid><description>&lt;p&gt;With the &lt;a href="https://docs.databricks.com/api/workspace/jobs/runnow"&gt;new Databricks jobs API 2.1&lt;/a&gt; you have different parameters depending on the kind of tasks you have in your workflow. Like: jar_params, sql_params, python_params, notebook_params&amp;hellip;&lt;/p&gt;
&lt;p&gt;And not always the airflow operator is ready to handle all of the. If we check the &lt;a href="https://airflow.apache.org/docs/apache-airflow-providers-databricks/stable/operators/run_now.html"&gt;current release of the DatabricksRunNowOperator&lt;/a&gt;, we can see that there is only support for:
notebook_params
python_params
python_named_parameters
jar_params
spark_submit_params
And not the query_params mentioned earlier. But there is a way of combining both, there is a param called &lt;em&gt;jsob&lt;/em&gt; that allows you to write the payload of a databricksrunnow and it will also merge the content of the JSON with your named_params!&lt;/p&gt;</description></item><item><title>Enabling Unity Catalog</title><link>https://adrianabreu.com/blog/2023-05-23-enabling-unity-catalog/</link><pubDate>Tue, 23 May 2023 07:48:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2023-05-23-enabling-unity-catalog/</guid><description>&lt;p&gt;I&amp;rsquo;ve spent the last few weeks setting up the unity catalog for my company. It&amp;rsquo;s been an extremely tiring process. And there are several concepts to bring here. My main point is to have a clear view of the requirements.&lt;/p&gt;
&lt;p&gt;Disclaimer: as of today with &lt;a href="https://github.com/databricks/terraform-provider-databricks"&gt;https://github.com/databricks/terraform-provider-databricks&lt;/a&gt; release 1.17.0, some steps should be done in an &amp;ldquo;awkward way&amp;rdquo; that is, the account API does not expose the catalog&amp;rsquo;s endpoint and should be done through a workspace.&lt;/p&gt;</description></item><item><title>Testing Databricks Photon</title><link>https://adrianabreu.com/blog/2022-08-12-testing-photon-engine/</link><pubDate>Fri, 12 Aug 2022 09:52:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2022-08-12-testing-photon-engine/</guid><description>&lt;p&gt;I was a bit skeptical about photon since I realized that it cost about double the amount of DBU, required specifically optimized machines and did not support UDFs (it was my main target).&lt;/p&gt;
&lt;p&gt;From the Databricks Official Docs:&lt;/p&gt;
&lt;h1 id="limitations"&gt;&lt;strong&gt;Limitations&lt;/strong&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Does not support Spark Structured Streaming.&lt;/li&gt;
&lt;li&gt;Does not support UDFs.&lt;/li&gt;
&lt;li&gt;Does not support RDD APIs.&lt;/li&gt;
&lt;li&gt;Not expected to improve short-running queries (&amp;lt;2 seconds), for example, queries against small amounts of data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://docs.databricks.com/runtime/photon.html"&gt;Photon runtime&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Databricks Cluster Management</title><link>https://adrianabreu.com/blog/2022-07-30-databricks-cluster-management/</link><pubDate>Sat, 30 Jul 2022 13:52:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2022-07-30-databricks-cluster-management/</guid><description>&lt;p&gt;For the last few months, I&amp;rsquo;ve been into ETL optimization. Most of the changes were as dramatic as moving tables from ORC to delta revamping the partition strategy to some as simple as upgrading the runtime version to 10.4 so the ETL starts using low-shuffle merge.&lt;/p&gt;
&lt;p&gt;But at my job, we have a &lt;em&gt;lot&lt;/em&gt; of jobs. Each ETL can be easily launched at *30 with different parameters so I wanted to dig into the most effective strategy for it.&lt;/p&gt;</description></item><item><title>Reading firebase data</title><link>https://adrianabreu.com/blog/2022-07-01-reading-firebase-data/</link><pubDate>Fri, 01 Jul 2022 07:28:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2022-07-01-reading-firebase-data/</guid><description>&lt;p&gt;Firebase is a common component nowadays for most mobile apps. And it can provide some useful insights, for example in my previous company we use it to detect where the people left at the initial app wizard. (We could measure it).&lt;/p&gt;
&lt;p&gt;It is quite simple to export your data to BigQuery: &lt;a href="https://firebase.google.com/docs/projects/bigquery-export"&gt;https://firebase.google.com/docs/projects/bigquery-export&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;But maybe your lake is in AWS or Azure. In the next lines, I will try to explain how to load the data in your lake and some improvements we have applied.&lt;/p&gt;</description></item></channel></rss>