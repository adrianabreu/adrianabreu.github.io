<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DataBricks on Adrián Abreu</title><link>https://adrianabreu.github.io/tags/databricks/</link><description>Recent content in DataBricks on Adrián Abreu</description><generator>Hugo -- gohugo.io</generator><language>es-ES</language><copyright>2017 Adrián Abreu powered by Hugo and Kiss Theme</copyright><lastBuildDate>Mon, 15 Aug 2022 09:52:32 +0000</lastBuildDate><atom:link href="https://adrianabreu.github.io/tags/databricks/index.xml" rel="self" type="application/rss+xml"/><item><title>Optimizing Spark</title><link>https://adrianabreu.github.io/blog/2023-08-12-15-optimizing-spark-i/</link><pubDate>Mon, 15 Aug 2022 09:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2023-08-12-15-optimizing-spark-i/</guid><description>Últimamente me he centrado en mejorar mis habilidades con Spark y he aprovechado para hacer algunos trainings de databricks. (Que por cierto ha sacado Beacons, un programa de reconocimiento para sus colaboradores y ha mencionado algunos nombres muy grandes por ahí).
Y en estos cursos está optimizing spark, que simplifica y explica de una forma bastante sencilla los problemas de rendimientos que ocurren en el mundo de big data. A estos problemas se les denomina las 5s:</description></item><item><title>Optimizing Spark II</title><link>https://adrianabreu.github.io/blog/2023-08-12-16-optimizing-spark-ii/</link><pubDate>Mon, 15 Aug 2022 09:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2023-08-12-16-optimizing-spark-ii/</guid><description>Continuando con la lista de optimizaciones en spark tneemos el spill.
Hacer spill no es más que persistir un rdd en disco, ya que, sus datos no caben en memoria.
Existen varias causas, la más sencilla de pensar es hacer un explode un array donde nuestras columnas crecen de forma exponencial.
Cuando el spill ocurre se puede identificar por dos valores que siempre van de la mano:
Spill (Memory) Spill (Disk) (Estas columnas solo aparecen en la spark ui si hay spill).</description></item><item><title>Testing Databricks Photon</title><link>https://adrianabreu.github.io/blog/2022-08-12-testing-photon-engine/</link><pubDate>Fri, 12 Aug 2022 09:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2022-08-12-testing-photon-engine/</guid><description>I was a bit skeptical about photon since I realized that it cost about double the amount of DBU, required specifically optimized machines and did not support UDFs (it was my main target).
From the Databricks Official Docs:
Limitations Does not support Spark Structured Streaming. Does not support UDFs. Does not support RDD APIs. Not expected to improve short-running queries (&amp;lt;2 seconds), for example, queries against small amounts of data. Photon runtime</description></item><item><title>Databricks Cluster Management</title><link>https://adrianabreu.github.io/blog/2022-07-30-databricks-cluster-management/</link><pubDate>Sat, 30 Jul 2022 13:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2022-07-30-databricks-cluster-management/</guid><description>For the last few months, I&amp;rsquo;ve been into ETL optimization. Most of the changes were as dramatic as moving tables from ORC to delta revamping the partition strategy to some as simple as upgrading the runtime version to 10.4 so the ETL starts using low-shuffle merge.
But at my job, we have a lot of jobs. Each ETL can be easily launched at *30 with different parameters so I wanted to dig into the most effective strategy for it.</description></item><item><title>Reading firebase data</title><link>https://adrianabreu.github.io/blog/2022-07-01-reading-firebase-data/</link><pubDate>Fri, 01 Jul 2022 07:28:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2022-07-01-reading-firebase-data/</guid><description>Firebase is a common component nowadays for most mobile apps. And it can provide some useful insights, for example in my previous company we use it to detect where the people left at the initial app wizard. (We could measure it).
It is quite simple to export your data to BigQuery: https://firebase.google.com/docs/projects/bigquery-export
But maybe your lake is in AWS or Azure. In the next lines, I will try to explain how to load the data in your lake and some improvements we have applied.</description></item></channel></rss>