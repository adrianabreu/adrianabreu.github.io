<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Datalake on Adrián Abreu</title><link>https://adrianabreu.github.io/tags/datalake/</link><description>Recent content in Datalake on Adrián Abreu</description><generator>Hugo -- gohugo.io</generator><language>es-ES</language><copyright>2017 Adrián Abreu powered by Hugo and Kiss Theme</copyright><lastBuildDate>Tue, 25 Aug 2020 17:22:32 +0000</lastBuildDate><atom:link href="https://adrianabreu.github.io/tags/datalake/index.xml" rel="self" type="application/rss+xml"/><item><title>Detectando ficheros pequenos Spark</title><link>https://adrianabreu.github.io/blog/2020-08-25-detectando-ficheros-pequenos/</link><pubDate>Tue, 25 Aug 2020 17:22:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-08-25-detectando-ficheros-pequenos/</guid><description>Uno de los mayores problemas de rendimiento que podemos encontrar en los datalake es tener que mover una enorme cantidad de ficheros pequeños, por el overhead que eso representa en las transacciones. Este post de databricks recomendada https://forums.databricks.com/questions/101/what-is-an-optimal-size-for-file-partitions-using.html que se crearan ficheros de 1GB parquet.
Sin embargo mucha gente no sabe como detectar esto. Hace poco estuve jugando con un notebook y usando simplemente las herramientas del dbutils pude clasificar los ficheros que tenia en las entidades del datalake en múltiples categorías, así podría estimar cuantos ficheros había en un rango de tiempo.</description></item></channel></rss>