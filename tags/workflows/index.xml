<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Workflows on Adrián Abreu</title><link>https://adrianabreu.github.com/tags/workflows/</link><description>Recent content in Workflows on Adrián Abreu</description><generator>Hugo -- gohugo.io</generator><language>es-ES</language><copyright>2017-2022 Adrián Abreu powered by Hugo and Kiss Theme</copyright><lastBuildDate>Fri, 28 Jul 2023 16:00:32 +0000</lastBuildDate><atom:link href="https://adrianabreu.github.com/tags/workflows/index.xml" rel="self" type="application/rss+xml"/><item><title>Adding extra params on DatabricksRunNowOperator</title><link>https://adrianabreu.github.com/blog/2023-07-28-extra_params_databricksrunnow/</link><pubDate>Fri, 28 Jul 2023 16:00:32 +0000</pubDate><guid>https://adrianabreu.github.com/blog/2023-07-28-extra_params_databricksrunnow/</guid><description>With the new Databricks jobs API 2.1 you have different parameters depending on the kind of tasks you have in your workflow. Like: jar_params, sql_params, python_params, notebook_params&amp;hellip;
And not always the airflow operator is ready to handle all of the. If we check the current release of the DatabricksRunNowOperator, we can see that there is only support for: notebook_params python_params python_named_parameters jar_params spark_submit_params And not the query_params mentioned earlier. But there is a way of combining both, there is a param called jsob that allows you to write the payload of a databricksrunnow and it will also merge the content of the JSON with your named_params!</description></item></channel></rss>