<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Workflows on Adrián Abreu</title><link>https://adrianabreu.com/tags/workflows/</link><description>Recent content in Workflows on Adrián Abreu</description><generator>Hugo</generator><language>es-ES</language><copyright>2017-2024 Adrián Abreu powered by Hugo and Kiss Theme</copyright><lastBuildDate>Fri, 28 Jul 2023 16:00:32 +0000</lastBuildDate><atom:link href="https://adrianabreu.com/tags/workflows/index.xml" rel="self" type="application/rss+xml"/><item><title>Adding extra params on DatabricksRunNowOperator</title><link>https://adrianabreu.com/blog/2023-07-28-extra_params_databricksrunnow/</link><pubDate>Fri, 28 Jul 2023 16:00:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2023-07-28-extra_params_databricksrunnow/</guid><description>&lt;p>With the &lt;a href="https://docs.databricks.com/api/workspace/jobs/runnow">new Databricks jobs API 2.1&lt;/a> you have different parameters depending on the kind of tasks you have in your workflow. Like: jar_params, sql_params, python_params, notebook_params&amp;hellip;&lt;/p>
&lt;p>And not always the airflow operator is ready to handle all of the. If we check the &lt;a href="https://airflow.apache.org/docs/apache-airflow-providers-databricks/stable/operators/run_now.html">current release of the DatabricksRunNowOperator&lt;/a>, we can see that there is only support for:
notebook_params
python_params
python_named_parameters
jar_params
spark_submit_params
And not the query_params mentioned earlier. But there is a way of combining both, there is a param called &lt;em>jsob&lt;/em> that allows you to write the payload of a databricksrunnow and it will also merge the content of the JSON with your named_params!&lt;/p></description></item></channel></rss>