<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data Factory on Adrián Abreu</title><link>https://adrianabreu.github.com/tags/data-factory/</link><description>Recent content in Data Factory on Adrián Abreu</description><generator>Hugo -- gohugo.io</generator><language>es-ES</language><copyright>2017-2022 Adrián Abreu powered by Hugo and Kiss Theme</copyright><lastBuildDate>Thu, 01 Oct 2020 10:12:32 +0000</lastBuildDate><atom:link href="https://adrianabreu.github.com/tags/data-factory/index.xml" rel="self" type="application/rss+xml"/><item><title>Jugando con Data Factory</title><link>https://adrianabreu.github.com/blog/2020-10-01-jugando-con-df/</link><pubDate>Thu, 01 Oct 2020 10:12:32 +0000</pubDate><guid>https://adrianabreu.github.com/blog/2020-10-01-jugando-con-df/</guid><description>Sorprendentemente, hasta ahora, no había tenido la posibilidad de trabajar con data factory, sólo lo habia usado para algunas migraciones de datos.
Sin embargo, tras estabilizar un proyecto y consolidar su nueva etapa, necesitabamos simplificar la solución implementada para migrar datos.
Una representación sencilla de la arquitectura actual sería:
En un flujo muy sencillo sería esto:
La etl escribe un fichero csv con spark en un directorio de un blob storage.</description></item></channel></rss>