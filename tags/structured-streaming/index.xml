<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Structured Streaming on Adrián Abreu</title><link>https://adrianabreu.github.io/tags/structured-streaming/</link><description>Recent content in Structured Streaming on Adrián Abreu</description><generator>Hugo</generator><language>es-ES</language><copyright>2017-2024 Adrián Abreu powered by Hugo and Kiss Theme</copyright><lastBuildDate>Fri, 27 Oct 2023 12:06:32 +0000</lastBuildDate><atom:link href="https://adrianabreu.github.io/tags/structured-streaming/index.xml" rel="self" type="application/rss+xml"/><item><title>Tweaking Spark Kafka</title><link>https://adrianabreu.github.io/blog/2023-10-27-tweaking-spark-kafka/</link><pubDate>Fri, 27 Oct 2023 12:06:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2023-10-27-tweaking-spark-kafka/</guid><description>Well, I&amp;rsquo;m facing a huge interesting case. I&amp;rsquo;m working at Wallbox where we need to deal with billions of rows every day. Now we need to use Spark for some Kafka filtering and publish the results into different topics according to some rules.
I won&amp;rsquo;t dig deep into the logic except for performance-related stuff, let&amp;rsquo;s try to increase the processing speed.
When reading from Kafka you usually get 1 task per partition, so if you have 6 partitions and 48 cores you are not using 87.</description></item></channel></rss>