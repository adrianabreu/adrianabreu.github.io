<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Spark on Adrián Abreu</title><link>https://adrianabreu.github.io/tags/spark/</link><description>Recent content in Spark on Adrián Abreu</description><generator>Hugo</generator><language>es-ES</language><copyright>2017-2024 Adrián Abreu powered by Hugo and Kiss Theme</copyright><lastBuildDate>Fri, 22 Mar 2024 18:06:32 +0000</lastBuildDate><atom:link href="https://adrianabreu.github.io/tags/spark/index.xml" rel="self" type="application/rss+xml"/><item><title>Developing on windows</title><link>https://adrianabreu.github.io/blog/2024-03-22-developing-on-windows/</link><pubDate>Fri, 22 Mar 2024 18:06:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2024-03-22-developing-on-windows/</guid><description>Over the years, I&amp;rsquo;ve been using MacOS at work and Ubuntu at home for my development tasks. However, my Lenovo P1 Gen 3 laptop didn&amp;rsquo;t work well with Linux, leading to frequent issues with the camera and graphics (screen flickering, I&amp;rsquo;m looking at you, and it hurts).
I&amp;rsquo;ve triend Windows Subsystem for Linux (WSL) but it was quite bad to be honest. But as I&amp;rsquo;ve heard of WSL2 and WSLg, I decided to give it another shot.</description></item><item><title>Querying the databricks api</title><link>https://adrianabreu.github.io/blog/2024-01-26-querying-the-databricks-api/</link><pubDate>Fri, 26 Jan 2024 09:06:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2024-01-26-querying-the-databricks-api/</guid><description>Exploring databricks SQL usage
At my company, we adopted databricks SQL for most of our users. Some users have developed applications that use the JDBC connector, some users have built their dashboards, and some users write plain ad-hoc queries.
We wanted to know what they queried, so we tried to use Unity Catalog&amp;rsquo;s insights, but it wasn&amp;rsquo;t enough for our case. We work with IOT and we are interested in what filters they apply within our tables.</description></item><item><title>Tweaking Spark Kafka</title><link>https://adrianabreu.github.io/blog/2023-10-27-tweaking-spark-kafka/</link><pubDate>Fri, 27 Oct 2023 12:06:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2023-10-27-tweaking-spark-kafka/</guid><description>Well, I&amp;rsquo;m facing a huge interesting case. I&amp;rsquo;m working at Wallbox where we need to deal with billions of rows every day. Now we need to use Spark for some Kafka filtering and publish the results into different topics according to some rules.
I won&amp;rsquo;t dig deep into the logic except for performance-related stuff, let&amp;rsquo;s try to increase the processing speed.
When reading from Kafka you usually get 1 task per partition, so if you have 6 partitions and 48 cores you are not using 87.</description></item><item><title>Duplicates with delta, how can it be?</title><link>https://adrianabreu.github.io/blog/2023-03-20-delta-duplicates/</link><pubDate>Mon, 20 Mar 2023 09:50:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2023-03-20-delta-duplicates/</guid><description>Long time without writing! On highlights: I left my job at Schwarz It in December last year, and now I&amp;rsquo;m a full-time employee at Wallbox! I&amp;rsquo;m really happy with my new job, and I&amp;rsquo;ve experienced interesting stuff. This one was just one of these strange cases where you start doubting the compiler.
Context One of my main tables represents sensor measures from our chargers with millisecond precision. The numbers are quite high, we are talking over 2 billion rows per day.</description></item><item><title>Optimizing Spark</title><link>https://adrianabreu.github.io/blog/2024-12-12-15-optimizing-spark-i/</link><pubDate>Mon, 15 Aug 2022 09:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2024-12-12-15-optimizing-spark-i/</guid><description>Últimamente me he centrado en mejorar mis habilidades con Spark y he aprovechado para hacer algunos trainings de databricks. (Que por cierto ha sacado Beacons, un programa de reconocimiento para sus colaboradores y ha mencionado algunos nombres muy grandes por ahí).
Y en estos cursos está optimizing spark, que simplifica y explica de una forma bastante sencilla los problemas de rendimientos que ocurren en el mundo de big data. A estos problemas se les denomina las 5s:</description></item><item><title>Optimizing Spark II</title><link>https://adrianabreu.github.io/blog/2024-12-12-16-optimizing-spark-ii/</link><pubDate>Mon, 15 Aug 2022 09:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2024-12-12-16-optimizing-spark-ii/</guid><description>Continuando con la lista de optimizaciones en spark tneemos el spill.
Hacer spill no es más que persistir un rdd en disco, ya que, sus datos no caben en memoria.
Existen varias causas, la más sencilla de pensar es hacer un explode un array donde nuestras columnas crecen de forma exponencial.
Cuando el spill ocurre se puede identificar por dos valores que siempre van de la mano:
Spill (Memory) Spill (Disk) (Estas columnas solo aparecen en la spark ui si hay spill).</description></item><item><title>Testing Databricks Photon</title><link>https://adrianabreu.github.io/blog/2022-08-12-testing-photon-engine/</link><pubDate>Fri, 12 Aug 2022 09:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2022-08-12-testing-photon-engine/</guid><description>I was a bit skeptical about photon since I realized that it cost about double the amount of DBU, required specifically optimized machines and did not support UDFs (it was my main target).
From the Databricks Official Docs:
Limitations Does not support Spark Structured Streaming. Does not support UDFs. Does not support RDD APIs. Not expected to improve short-running queries (&amp;lt;2 seconds), for example, queries against small amounts of data. Photon runtime</description></item><item><title>Databricks Cluster Management</title><link>https://adrianabreu.github.io/blog/2022-07-30-databricks-cluster-management/</link><pubDate>Sat, 30 Jul 2022 13:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2022-07-30-databricks-cluster-management/</guid><description>For the last few months, I&amp;rsquo;ve been into ETL optimization. Most of the changes were as dramatic as moving tables from ORC to delta revamping the partition strategy to some as simple as upgrading the runtime version to 10.4 so the ETL starts using low-shuffle merge.
But at my job, we have a lot of jobs. Each ETL can be easily launched at *30 with different parameters so I wanted to dig into the most effective strategy for it.</description></item><item><title>Associate Spark Developer Certification</title><link>https://adrianabreu.github.io/spark-certification/2022-07-21-passed-certification/</link><pubDate>Thu, 21 Jul 2022 12:16:32 +0000</pubDate><guid>https://adrianabreu.github.io/spark-certification/2022-07-21-passed-certification/</guid><description>Yesterday I took (and passed with more than 90% yay!) the Associate Spark Developer Certificaton. And before I forget I want to share my experience:
In general:
First of all, I needed to install Windows as there was no Linux support for the control software used during the exam. Secondly, you need to disable both the antivirus and the firewall before joining. I didn&amp;rsquo;t disable the antivirus and the technician contacted me as there was a problem with the webcam despite I was able to see myself.</description></item><item><title>Reading firebase data</title><link>https://adrianabreu.github.io/blog/2022-07-01-reading-firebase-data/</link><pubDate>Fri, 01 Jul 2022 07:28:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2022-07-01-reading-firebase-data/</guid><description>Firebase is a common component nowadays for most mobile apps. And it can provide some useful insights, for example in my previous company we use it to detect where the people left at the initial app wizard. (We could measure it).
It is quite simple to export your data to BigQuery: https://firebase.google.com/docs/projects/bigquery-export
But maybe your lake is in AWS or Azure. In the next lines, I will try to explain how to load the data in your lake and some improvements we have applied.</description></item><item><title>Qbeast</title><link>https://adrianabreu.github.io/blog/2022-06-30-qbeast/</link><pubDate>Thu, 30 Jun 2022 07:28:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2022-06-30-qbeast/</guid><description>A few days ago I ran into Qbeast which is an open-source project on top of delta lake I needed to dig into.
This introductory post explains it quite well: https://qbeast.io/qbeast-format-enhanced-data-lakehouse/
The project is quite good and it seems helpful if you need to write your custom data source as everything is documented. And well as I&amp;rsquo;m in love with note-taking I want to dig into the following three topics:</description></item><item><title>Spark Dates</title><link>https://adrianabreu.github.io/spark-certification/2022-06-29-spark-dates/</link><pubDate>Wed, 29 Jun 2022 15:43:22 +0000</pubDate><guid>https://adrianabreu.github.io/spark-certification/2022-06-29-spark-dates/</guid><description>I can perfectly describe this as the scariest part of the exam. I&amp;rsquo;m used to working with dates but I&amp;rsquo;m especially used to suffering from the typical UTC / not UTC / summer time hours difference.
I will try to make some simple exercises for this, the idea would be:
We have some sales data and god knows how the business people love to refresh super fast their dashboards on Databricks SQL.</description></item><item><title>Spark Cert Exam Practice</title><link>https://adrianabreu.github.io/spark-certification/2022-06-28-databricks-practice-exam/</link><pubDate>Tue, 28 Jun 2022 13:43:22 +0000</pubDate><guid>https://adrianabreu.github.io/spark-certification/2022-06-28-databricks-practice-exam/</guid><description>--- primary_color: orange secondary_color: lightgray text_color: black shuffle_questions: false --- ## Which of the following statements about the Spark driver is incorrect? - [ ] The Spark driver is the node in which the Spark application's main method runs to ordinate the Spark application. - [X] The Spark driver is horizontally scaled to increase overall processing throughput. - [ ] The Spark driver contains the SparkContext object. - [ ] The Spark driver is responsible for scheduling the execution of data by various worker nodes in cluster mode.</description></item><item><title>Spark User Defined Functions</title><link>https://adrianabreu.github.io/spark-certification/2022-06-19-spark-udf-udaf/</link><pubDate>Sun, 19 Jun 2022 14:43:22 +0000</pubDate><guid>https://adrianabreu.github.io/spark-certification/2022-06-19-spark-udf-udaf/</guid><description>Sometimes we need to execute arbitrary Scala code on Spark. We may need to use an external library or so on. For that, we have the UDF, which accepts and return one or more columns.
When we have a function we need to register it on Spark so we can use it on our worker machines. If you are using Scala or Java, the udf can run inside the Java Virtual Machine so there&amp;rsquo;s a little extra penalty.</description></item><item><title>Spark DataSources</title><link>https://adrianabreu.github.io/spark-certification/2022-06-11-spark-data-sources/</link><pubDate>Sat, 11 Jun 2022 16:43:22 +0000</pubDate><guid>https://adrianabreu.github.io/spark-certification/2022-06-11-spark-data-sources/</guid><description>As estated in the structured api section, Spark supports a lot of sources with a lot of options. There is no other goal for this post than to clarify how the most common ones work and how they will be converted to DataFrames.
First, all the supported sources are listed here: https://spark.apache.org/docs/latest/sql-data-sources.html
And we can focus on the typical ones: JSON, CSV and Parquet (as those are the typical format on open-source data).</description></item><item><title>Spark Dataframes</title><link>https://adrianabreu.github.io/spark-certification/2022-06-10-spark-structured-api/</link><pubDate>Fri, 10 Jun 2022 17:02:32 +0000</pubDate><guid>https://adrianabreu.github.io/spark-certification/2022-06-10-spark-structured-api/</guid><description>Spark was initially released for dealing with a particular type of data called RDD. Nowadays we work with abstract structures on top of it, and the following tables summarize them.
Type Description Advantages Datasets Structured composed of a list of where you can specify your custom class (only Scala) Type-safe operations, support for operations that cannot be expressed otherwise. Dataframes Datasets of type Row (a generic spark type) Allow optimizations and are more flexible SQL tables and views Same as Dataframes but in the scope of databases instead of programming languages Let&amp;rsquo;s dig into the Dataframes.</description></item><item><title>Spark Execution</title><link>https://adrianabreu.github.io/spark-certification/2022-06-08-spark-execution/</link><pubDate>Wed, 08 Jun 2022 17:02:32 +0000</pubDate><guid>https://adrianabreu.github.io/spark-certification/2022-06-08-spark-execution/</guid><description>Spark provides an api and an engine, that engine is responsible for analyzing the code and performing several optimizations. But how does this work? We can do two kinds of operations with Spark, transformations and actions.
Transformations are operations on top of the data that modify the data but do not yield a result directly, that is because they all are lazily evaluated so, you can add new columns, filter rows, or perform some computations that won&amp;rsquo;t be executed immediately.</description></item><item><title>Spark Architecture</title><link>https://adrianabreu.github.io/spark-certification/2022-06-07-spark-architecture/</link><pubDate>Tue, 07 Jun 2022 17:02:32 +0000</pubDate><guid>https://adrianabreu.github.io/spark-certification/2022-06-07-spark-architecture/</guid><description>Spark works on top of a cluster supervised by a cluster manager. The later is responsible of:
Tracking resource allocation across all applications running on the cluster. Monitoring the health of all the nodes. Inside each node there is a node manager which is responsible to track each node health and resources and inform the cluster manager.
C l u s t e r M a n a g e r N N N o o o d d d e e e M M M a a a n n n a a a g g g e e e r r r When we run a Spark application we generate processes inside the cluster where one node will act as a Driver and the rest will be Workers.</description></item><item><title>Multiplying rows in Spark</title><link>https://adrianabreu.github.io/blog/2021-11-11-multiplying-rows-in-spark/</link><pubDate>Thu, 11 Nov 2021 18:32:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2021-11-11-multiplying-rows-in-spark/</guid><description>Earlier this week I checked on a Pull Request that bothered me since I saw it from the first time. Let&amp;rsquo;s say we work for a bank and we are going to give cash to our clients if they get some people to join our bank.
And we have an advertising campaign definition like this:
campaign_id inviter_cash receiver_cash FakeBank001 50 30 FakeBank002 40 20 FakeBank003 30 20 And then our BI teams defines the schema they want for their dashboards.</description></item><item><title>Regex 101</title><link>https://adrianabreu.github.io/blog/2021-11-01-regex-101/</link><pubDate>Fri, 05 Nov 2021 16:39:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2021-11-01-regex-101/</guid><description>-You will spent your whole life relearning regex, there is a beginning, but never and end.
Last year I participated in some small code problems and practised some regex. I got used to it and feel quite good at it.
And today I had to use it again. I had the following dataframe:
product attributes 1 (SIZE-36) 2 (COLOR-RED) 3 (SIZE-38, COLOR-BLUE) 4 (COLOR-GREEN, SIZE-39) A wonderful set of string merged with properties that could vary.</description></item><item><title>Exportando los datos de firebase</title><link>https://adrianabreu.github.io/blog/2021-05-06-exporting-firebase-data/</link><pubDate>Thu, 06 May 2021 11:49:36 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2021-05-06-exporting-firebase-data/</guid><description>Si trabajamos analizando los datos de una aplicación móvil es muy probable que esté integrado algún sistema para trackear los eventos de la app. Y entre ellos, uno de los más conocidos es Firebase.
Estos eventos contienen mucha información útil y nos permiten por ejemplo saber, un usuario que se ha ido cuanto tiempo ha usado la aplicación o cuantos dias han pasado.
O si realmente ha seguido el flujo de acciones que esperabamos (con un diagrama de sankey podríamos ver donde se han ido los usuarios).</description></item><item><title>Calcular el domingo de la semana</title><link>https://adrianabreu.github.io/blog/2020-09-02-calcular-el-domingo-correspondiente/</link><pubDate>Wed, 02 Sep 2020 10:12:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-09-02-calcular-el-domingo-correspondiente/</guid><description>A la hora de publicar reportes es común agrupar los datos por semanas. Otro motivo es alinearse con el negocio donde los cierres pueden producirse en días concretos, por ejemplo, un domingo.
En esos casos si tenemos los datos particionados por días nos interesa saber a que domingo correspondería cada uno de los datos.
Los que venimos de otros entornos tendemos a pensar en esas complicadas librerías de fechas (moment.js, jodatime, etc).</description></item><item><title>Detectando ficheros pequenos Spark</title><link>https://adrianabreu.github.io/blog/2020-08-25-detectando-ficheros-pequenos/</link><pubDate>Tue, 25 Aug 2020 17:22:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-08-25-detectando-ficheros-pequenos/</guid><description>Uno de los mayores problemas de rendimiento que podemos encontrar en los datalake es tener que mover una enorme cantidad de ficheros pequeños, por el overhead que eso representa en las transacciones. Este post de databricks recomendada https://forums.databricks.com/questions/101/what-is-an-optimal-size-for-file-partitions-using.html que se crearan ficheros de 1GB parquet.
Sin embargo mucha gente no sabe como detectar esto. Hace poco estuve jugando con un notebook y usando simplemente las herramientas del dbutils pude clasificar los ficheros que tenia en las entidades del datalake en múltiples categorías, así podría estimar cuantos ficheros había en un rango de tiempo.</description></item><item><title>Spark windows functions (I)</title><link>https://adrianabreu.github.io/blog/2020-08-11-spark-windows-functions/</link><pubDate>Tue, 11 Aug 2020 18:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-08-11-spark-windows-functions/</guid><description>En analítica, es muy común hacer uso de las funciones de ventana para distintos cálculos. Hace poco me encontré con un pequeño problema cuya solución mejoró muchísimo al usar las funciones de ventana, demos un poco de contexto.
Tenemos una dimensión de usuarios donde los usuarios se van registrando con una fecha y tenemos una tabla de ventas donde tenemos las ventas globales para cada día
Y lo que queremos dar es una visión de cómo cada día evoluciona el programa, para ello se quiere que cada día estén tanto las ventas acumuladas como los registros acumulados.</description></item><item><title>Spark windows functions (II)</title><link>https://adrianabreu.github.io/blog/2025-01-01-spark-windows-functions-ii/</link><pubDate>Tue, 11 Aug 2020 18:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2025-01-01-spark-windows-functions-ii/</guid><description>En el post anterior pudimos utilizar las funciones de ventanas para realizar agregados de sumas sobre ventanas temporales. Ahora, me gustaría utilizar otro ejemplo de analítica: Comparar con datos previos.
Pongamos que queremos analizar las ventas de un producto en diversos periodos de tiempo. Es decir, nos interesa saber si ahora vende más o menos que antes.
Para ello partiremos de una tabla de ventas:
DateKey ProductId Sales Ahora que tenemos esto nos interesaría agrupar para cada producto sus ventas</description></item><item><title>Conceptos básicos de Spark</title><link>https://adrianabreu.github.io/blog/2019-11-09-spark-concepts-basicos/</link><pubDate>Sat, 09 Nov 2019 19:22:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2019-11-09-spark-concepts-basicos/</guid><description>Nota del autor: Todos los contenidos de este artículo son extractos del libro &amp;ldquo;The Data Engineer&amp;rsquo;s Guide to Apache Spark&amp;rdquo; que puedes descargar desde la pagina de databricks: https://databricks.com/lp/ebook/data-engineer-spark-guide
Preludio: Cluster: Un cluster no es más que un conjunto de máquinas trabajando de forma coordinada. Un cluster de Spark se compone de nodos. Uno actúa como DRIVER y es el punto de entrada para el código del usuario. Los otros actúan como EXECUTOR que seran los encargados de realizar las operaciones.</description></item><item><title>Empezando en Spark con Docker</title><link>https://adrianabreu.github.io/blog/2019-11-10-empezando-en-spark-con-docker/</link><pubDate>Sat, 09 Nov 2019 19:22:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2019-11-10-empezando-en-spark-con-docker/</guid><description>A pesar de haber leído guías tan buenas como:
https://medium.com/@bogdan.cojocar/how-to-run-scala-and-spark-in-the-jupyter-notebook-328a80090b3b
https://medium.com/@singhpraveen2010/install-apache-spark-and-configure-with-jupyter-notebook-in-10-minutes-ae120ebca597
Se me ha hecho cuesta arriba el poder conectar un notebook de jupyter y utilizar Scala. Entre configurar el apache toree para poder usar scala en los notebooks y algún error luego en spark al usarlo desde IntelliJ, me he acabado rindiendo.
Nota del autor: Como disclaimer esto ocurre probablemente porque estoy en Manjaro y mi version de Scala es incompatible.</description></item><item><title>Correlated subqueries</title><link>https://adrianabreu.github.io/blog/2019-09-26-correlated-subqueries/</link><pubDate>Thu, 26 Sep 2019 20:43:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2019-09-26-correlated-subqueries/</guid><description>Llevo un par de meses viendo como la mayoría de esfuerzos en el proyecto en el que estoy se centran en evitar los joins en las distintas capas de análisis. Aprovechando las capacidades de spark se busca tener las estructuras muy desnormalizadas y se había &amp;ldquo;endemoniado&amp;rdquo; al join considerarlo perjudicial.
Tanto es así que llevo un par de días peleando con una pieza de código que me ha sorprendido. Partiendo de una tabla de hechos que agrupa datos para un periodo a hasta b, se quiere que se &amp;ldquo;colapsen&amp;rdquo; los datos de hace 14 días.</description></item></channel></rss>