<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Spark on Adrián Abreu</title><link>https://adrianabreu.github.io/tags/spark/</link><description>Recent content in Spark on Adrián Abreu</description><generator>Hugo -- gohugo.io</generator><language>es-ES</language><copyright>2017 Adrián Abreu powered by Hugo and Kiss Theme</copyright><lastBuildDate>Fri, 05 Nov 2021 16:39:32 +0000</lastBuildDate><atom:link href="https://adrianabreu.github.io/tags/spark/index.xml" rel="self" type="application/rss+xml"/><item><title>Regex 101</title><link>https://adrianabreu.github.io/blog/2021-11-01-regex-101/</link><pubDate>Fri, 05 Nov 2021 16:39:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2021-11-01-regex-101/</guid><description>-You will spent your whole life relearning regex, there is a beginning, but never and end.
Last year I participated in some small code problems and practised some regex. I got used to it and feel quite good at it.
And today I had to use it again. I had the following dataframe:
product attributes 1 (SIZE-36) 2 (COLOR-RED) 3 (SIZE-38, COLOR-BLUE) 4 (COLOR-GREEN, SIZE-39) A wonderful set of string merged with properties that could vary.</description></item><item><title>Exportando los datos de firebase</title><link>https://adrianabreu.github.io/blog/2021-05-06-exporting-firebase-data/</link><pubDate>Thu, 06 May 2021 11:49:36 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2021-05-06-exporting-firebase-data/</guid><description>Si trabajamos analizando los datos de una aplicación móvil es muy probable que esté integrado algún sistema para trackear los eventos de la app. Y entre ellos, uno de los más conocidos es Firebase.
Estos eventos contienen mucha información útil y nos permiten por ejemplo saber, un usuario que se ha ido cuanto tiempo ha usado la aplicación o cuantos dias han pasado.
O si realmente ha seguido el flujo de acciones que esperabamos (con un diagrama de sankey podríamos ver donde se han ido los usuarios).</description></item><item><title>Calcular el domingo de la semana</title><link>https://adrianabreu.github.io/blog/2020-09-02-calcular-el-domingo-correspondiente/</link><pubDate>Wed, 02 Sep 2020 10:12:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-09-02-calcular-el-domingo-correspondiente/</guid><description>A la hora de publicar reportes es común agrupar los datos por semanas. Otro motivo es alinearse con el negocio donde los cierres pueden producirse en días concretos, por ejemplo, un domingo.
En esos casos si tenemos los datos particionados por días nos interesa saber a que domingo correspondería cada uno de los datos.
Los que venimos de otros entornos tendemos a pensar en esas complicadas librerías de fechas (moment.js, jodatime, etc).</description></item><item><title>Detectando ficheros pequenos Spark</title><link>https://adrianabreu.github.io/blog/2020-08-25-detectando-ficheros-pequenos/</link><pubDate>Tue, 25 Aug 2020 17:22:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-08-25-detectando-ficheros-pequenos/</guid><description>Uno de los mayores problemas de rendimiento que podemos encontrar en los datalake es tener que mover una enorme cantidad de ficheros pequeños, por el overhead que eso representa en las transacciones. Este post de databricks recomendada https://forums.databricks.com/questions/101/what-is-an-optimal-size-for-file-partitions-using.html que se crearan ficheros de 1GB parquet.
Sin embargo mucha gente no sabe como detectar esto. Hace poco estuve jugando con un notebook y usando simplemente las herramientas del dbutils pude clasificar los ficheros que tenia en las entidades del datalake en múltiples categorías, así podría estimar cuantos ficheros había en un rango de tiempo.</description></item><item><title>Spark windows functions (I)</title><link>https://adrianabreu.github.io/blog/2020-08-11-spark-windows-functions/</link><pubDate>Tue, 11 Aug 2020 18:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-08-11-spark-windows-functions/</guid><description>En analítica, es muy común hacer uso de las funciones de ventana para distintos cálculos. Hace poco me encontré con un pequeño problema cuya solución mejoró muchísimo al usar las funciones de ventana, demos un poco de contexto.
Tenemos una dimensión de usuarios donde los usuarios se van registrando con una fecha y tenemos una tabla de ventas donde tenemos las ventas globales para cada día
Y lo que queremos dar es una visión de cómo cada día evoluciona el programa, para ello se quiere que cada día estén tanto las ventas acumuladas como los registros acumulados.</description></item><item><title>Spark windows functions (II)</title><link>https://adrianabreu.github.io/blog/2022-01-01-spark-windows-functions-ii/</link><pubDate>Tue, 11 Aug 2020 18:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2022-01-01-spark-windows-functions-ii/</guid><description>En el post anterior pudimos utilizar las funciones de ventanas para realizar agregados de sumas sobre ventanas temporales. Ahora, me gustaría utilizar otro ejemplo de analítica: Comparar con datos previos.
Pongamos que queremos analizar las ventas de un producto en diversos periodos de tiempo. Es decir, nos interesa saber si ahora vende más o menos que antes.
Para ello partiremos de una tabla de ventas:
DateKey ProductId Sales Ahora que tenemos esto nos interesaría agrupar para cada producto sus ventas</description></item><item><title>Conceptos básicos de Spark</title><link>https://adrianabreu.github.io/blog/2019-11-09-spark-concepts-basicos/</link><pubDate>Sat, 09 Nov 2019 19:22:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2019-11-09-spark-concepts-basicos/</guid><description>Nota del autor: Todos los contenidos de este artículo son extractos del libro &amp;ldquo;The Data Engineer&amp;rsquo;s Guide to Apache Spark&amp;rdquo; que puedes descargar desde la pagina de databricks: https://databricks.com/lp/ebook/data-engineer-spark-guide
Preludio: Cluster: Un cluster no es más que un conjunto de máquinas trabajando de forma coordinada. Un cluster de Spark se compone de nodos. Uno actúa como DRIVER y es el punto de entrada para el código del usuario. Los otros actúan como EXECUTOR que seran los encargados de realizar las operaciones.</description></item><item><title>Empezando en Spark con Docker</title><link>https://adrianabreu.github.io/blog/2019-11-10-empezando-en-spark-con-docker/</link><pubDate>Sat, 09 Nov 2019 19:22:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2019-11-10-empezando-en-spark-con-docker/</guid><description>A pesar de haber leído guías tan buenas como:
https://medium.com/@bogdan.cojocar/how-to-run-scala-and-spark-in-the-jupyter-notebook-328a80090b3b
https://medium.com/@singhpraveen2010/install-apache-spark-and-configure-with-jupyter-notebook-in-10-minutes-ae120ebca597
Se me ha hecho cuesta arriba el poder conectar un notebook de jupyter y utilizar Scala. Entre configurar el apache toree para poder usar scala en los notebooks y algún error luego en spark al usarlo desde IntelliJ, me he acabado rindiendo.
Nota del autor: Como disclaimer esto ocurre probablemente porque estoy en Manjaro y mi version de Scala es incompatible.</description></item><item><title>Correlated subqueries</title><link>https://adrianabreu.github.io/blog/2019-09-26-correlated-subqueries/</link><pubDate>Thu, 26 Sep 2019 20:43:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2019-09-26-correlated-subqueries/</guid><description>Llevo un par de meses viendo como la mayoría de esfuerzos en el proyecto en el que estoy se centran en evitar los joins en las distintas capas de análisis. Aprovechando las capacidades de spark se busca tener las estructuras muy desnormalizadas y se había &amp;ldquo;endemoniado&amp;rdquo; al join considerarlo perjudicial.
Tanto es así que llevo un par de días peleando con una pieza de código que me ha sorprendido. Partiendo de una tabla de hechos que agrupa datos para un periodo a hasta b, se quiere que se &amp;ldquo;colapsen&amp;rdquo; los datos de hace 14 días.</description></item></channel></rss>