<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Analytics on Adrián Abreu</title><link>https://adrianabreu.com/tags/analytics/</link><description>Recent content in Analytics on Adrián Abreu</description><generator>Hugo</generator><language>es-ES</language><copyright>2017-2024 Adrián Abreu powered by Hugo and Kiss Theme</copyright><lastBuildDate>Thu, 11 Nov 2021 18:32:32 +0000</lastBuildDate><atom:link href="https://adrianabreu.com/tags/analytics/index.xml" rel="self" type="application/rss+xml"/><item><title>Multiplying rows in Spark</title><link>https://adrianabreu.com/blog/2021-11-11-multiplying-rows-in-spark/</link><pubDate>Thu, 11 Nov 2021 18:32:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2021-11-11-multiplying-rows-in-spark/</guid><description>&lt;p>Earlier this week I checked on a Pull Request that bothered me since I saw it from the first time. Let&amp;rsquo;s say we work for a bank and we are going to give cash to our clients if they get some people to join our bank.&lt;/p>
&lt;p>And we have an advertising campaign definition like this:&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: left">campaign_id&lt;/th>
 &lt;th style="text-align: left">inviter_cash&lt;/th>
 &lt;th style="text-align: left">receiver_cash&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: left">FakeBank001&lt;/td>
 &lt;td style="text-align: left">50&lt;/td>
 &lt;td style="text-align: left">30&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">FakeBank002&lt;/td>
 &lt;td style="text-align: left">40&lt;/td>
 &lt;td style="text-align: left">20&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">FakeBank003&lt;/td>
 &lt;td style="text-align: left">30&lt;/td>
 &lt;td style="text-align: left">20&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>And then our BI teams defines the schema they want for their dashboards.&lt;/p></description></item><item><title>A funny bug</title><link>https://adrianabreu.com/blog/2021-07-21-funny-bug/</link><pubDate>Wed, 21 Jul 2021 18:52:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2021-07-21-funny-bug/</guid><description>&lt;p>Ayer mismo estaba intentando analizar unos data_types para completar una exportación al backend. La idea era sencilla, buscar para cada usuario la ultima información disponible en una tabla que representaba actualizaciones sobre su perfil.&lt;/p>
&lt;p>Como a su perfil podías &amp;ldquo;añadir&amp;rdquo;, &amp;ldquo;actualizar&amp;rdquo; y &amp;ldquo;eliminar&amp;rdquo; cosas, pues existian los tres tipos en la tabla. Para que la imaginemos mejor sería tal que así:&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: left">user_id&lt;/th>
 &lt;th style="text-align: left">favorite_stuff&lt;/th>
 &lt;th style="text-align: left">operation&lt;/th>
 &lt;th style="text-align: left">metadata&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: left">A&lt;/td>
 &lt;td style="text-align: left">Chocolate&lt;/td>
 &lt;td style="text-align: left">Add&lt;/td>
 &lt;td style="text-align: left">&amp;hellip;&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">A&lt;/td>
 &lt;td style="text-align: left">Chocolate&lt;/td>
 &lt;td style="text-align: left">Update&lt;/td>
 &lt;td style="text-align: left">&amp;hellip;&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">B&lt;/td>
 &lt;td style="text-align: left">Milk&lt;/td>
 &lt;td style="text-align: left">Remove&lt;/td>
 &lt;td style="text-align: left">&amp;hellip;&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">B&lt;/td>
 &lt;td style="text-align: left">Cornflakes&lt;/td>
 &lt;td style="text-align: left">Add&lt;/td>
 &lt;td style="text-align: left">&amp;hellip;&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>De tal manera que habría que combinar todos los eventos para saber cual es el perfil actual del usuario y aplicar cierta lógica. Sin embargo los eventos que habían llegado realmente eran así:&lt;/p></description></item><item><title>Exportando los datos de firebase</title><link>https://adrianabreu.com/blog/2021-05-06-exporting-firebase-data/</link><pubDate>Thu, 06 May 2021 11:49:36 +0000</pubDate><guid>https://adrianabreu.com/blog/2021-05-06-exporting-firebase-data/</guid><description>&lt;p>Si trabajamos analizando los datos de una aplicación móvil es muy probable que esté integrado algún sistema para trackear los eventos de la app. Y entre ellos, uno de los más conocidos es Firebase.&lt;/p>
&lt;p>Estos eventos contienen mucha información útil y nos permiten por ejemplo saber, un usuario que se ha ido cuanto tiempo ha usado la aplicación o cuantos dias han pasado.&lt;/p>
&lt;p>O si realmente ha seguido el flujo de acciones que esperabamos (con un diagrama de sankey podríamos ver donde se han ido los usuarios).&lt;/p></description></item><item><title>Calcular el domingo de la semana</title><link>https://adrianabreu.com/blog/2020-09-02-calcular-el-domingo-correspondiente/</link><pubDate>Wed, 02 Sep 2020 10:12:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2020-09-02-calcular-el-domingo-correspondiente/</guid><description>&lt;p>A la hora de publicar reportes es común agrupar los datos por semanas. Otro motivo es alinearse con el negocio donde los cierres pueden producirse en días concretos, por ejemplo, un domingo.&lt;/p>
&lt;p>En esos casos si tenemos los datos particionados por días nos interesa saber a que domingo correspondería cada uno de los datos.&lt;/p>
&lt;p>Los que venimos de otros entornos tendemos a pensar en esas complicadas librerías de fechas (moment.js, jodatime, etc). Incluso alguien podría pensar en extraer los datos del dataframe y procesarlo en local.&lt;/p></description></item><item><title>Detectando ficheros pequenos Spark</title><link>https://adrianabreu.com/blog/2020-08-25-detectando-ficheros-pequenos/</link><pubDate>Tue, 25 Aug 2020 17:22:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2020-08-25-detectando-ficheros-pequenos/</guid><description>&lt;p>Uno de los mayores problemas de rendimiento que podemos encontrar en los datalake es tener que mover una enorme cantidad de ficheros pequeños, por el overhead que eso representa en las transacciones.
Este post de databricks recomendada &lt;a href="https://forums.databricks.com/questions/101/what-is-an-optimal-size-for-file-partitions-using.html">https://forums.databricks.com/questions/101/what-is-an-optimal-size-for-file-partitions-using.html&lt;/a> que se crearan ficheros de 1GB parquet.&lt;/p>
&lt;p>Sin embargo mucha gente no sabe como detectar esto. Hace poco estuve jugando con un notebook y usando simplemente las herramientas del dbutils pude clasificar los ficheros que tenia en las entidades del datalake en múltiples categorías, así podría estimar cuantos ficheros había en un rango de tiempo.&lt;/p></description></item><item><title>Spark windows functions (I)</title><link>https://adrianabreu.com/blog/2020-08-11-spark-windows-functions/</link><pubDate>Tue, 11 Aug 2020 18:52:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2020-08-11-spark-windows-functions/</guid><description>&lt;p>En analítica, es muy común hacer uso de las funciones de ventana para distintos cálculos. Hace poco me encontré con un pequeño problema cuya solución mejoró muchísimo al usar las funciones de ventana, demos un poco de contexto.&lt;/p>
&lt;p>Tenemos una dimensión de usuarios donde los usuarios se van registrando con una fecha y tenemos una tabla de ventas donde tenemos las ventas globales para cada día&lt;/p>
&lt;p>Y lo que queremos dar es una visión de cómo cada día evoluciona el programa, para ello se quiere que cada día estén tanto las ventas acumuladas como los registros acumulados.&lt;/p></description></item><item><title>Correlated subqueries</title><link>https://adrianabreu.com/blog/2019-09-26-correlated-subqueries/</link><pubDate>Thu, 26 Sep 2019 20:43:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2019-09-26-correlated-subqueries/</guid><description>&lt;p>Llevo un par de meses viendo como la mayoría de esfuerzos en el proyecto en el que estoy se centran en evitar los joins en las distintas capas de análisis. Aprovechando las capacidades de spark se busca tener las estructuras muy desnormalizadas y se había &amp;ldquo;endemoniado&amp;rdquo; al join considerarlo perjudicial.&lt;/p>
&lt;p>Tanto es así que llevo un par de días peleando con una pieza de código que me ha sorprendido. Partiendo de una tabla de hechos que agrupa datos para un periodo a hasta b, se quiere que se &amp;ldquo;colapsen&amp;rdquo; los datos de hace 14 días. Será mejor con un ejemplo:&lt;/p></description></item><item><title>Datos I - Introducción al Datawarehousing</title><link>https://adrianabreu.com/blog/2019-02-05-data-i-introduccion-al-datawarehouse/</link><pubDate>Tue, 05 Feb 2019 14:43:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2019-02-05-data-i-introduccion-al-datawarehouse/</guid><description>&lt;p>En los últimos meses mi trabajo ha pivotado del mundo de la web al mundo de los datos. He entrado a participar en un proyecto de Data Warehouse y he acabado muy contento en él. Hace unos días mi cambio se oficializó completamente y ahora me he dado cuenta de que no solo tengo un mundo técnico ante mí, sino que además necesito consolidar algunas bases teóricas.&lt;/p>
&lt;p>Investigando la bibliografía, me han recomendado en Reddit: &lt;em>The Data Warehouse Toolkit, The Complete Guide to Dimensional Modeling 2nd Edition.&lt;/em>
Y el libro parece encajar perfectamente en el conocimiento que busco. Aun así, como todo, por necesidad, intentaré resumir en unas cuantas entradas el conocimiento que se puede obtener de este libro. El cual recomiendo encarecidamente.&lt;/p></description></item></channel></rss>