<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SQL on Adrián Abreu</title><link>https://adrianabreu.gitlab.io/tags/sql/</link><description>Recent content in SQL on Adrián Abreu</description><generator>Hugo -- gohugo.io</generator><language>es-ES</language><copyright>2017-2022 Adrián Abreu powered by Hugo and Kiss Theme</copyright><lastBuildDate>Mon, 20 Mar 2023 09:50:32 +0000</lastBuildDate><atom:link href="https://adrianabreu.gitlab.io/tags/sql/index.xml" rel="self" type="application/rss+xml"/><item><title>Duplicates with delta, how can it be?</title><link>https://adrianabreu.gitlab.io/blog/2023-03-20-delta-duplicates/</link><pubDate>Mon, 20 Mar 2023 09:50:32 +0000</pubDate><guid>https://adrianabreu.gitlab.io/blog/2023-03-20-delta-duplicates/</guid><description>Long time without writing! On highlights: I left my job at Schwarz It in December last year, and now I&amp;rsquo;m a full-time employee at Wallbox! I&amp;rsquo;m really happy with my new job, and I&amp;rsquo;ve experienced interesting stuff. This one was just one of these strange cases where you start doubting the compiler.
Context One of my main tables represents sensor measures from our chargers with millisecond precision. The numbers are quite high, we are talking over 2 billion rows per day.</description></item><item><title>A funny bug</title><link>https://adrianabreu.gitlab.io/blog/2021-07-21-funny-bug/</link><pubDate>Wed, 21 Jul 2021 18:52:32 +0000</pubDate><guid>https://adrianabreu.gitlab.io/blog/2021-07-21-funny-bug/</guid><description>Ayer mismo estaba intentando analizar unos data_types para completar una exportación al backend. La idea era sencilla, buscar para cada usuario la ultima información disponible en una tabla que representaba actualizaciones sobre su perfil.
Como a su perfil podías &amp;ldquo;añadir&amp;rdquo;, &amp;ldquo;actualizar&amp;rdquo; y &amp;ldquo;eliminar&amp;rdquo; cosas, pues existian los tres tipos en la tabla. Para que la imaginemos mejor sería tal que así:
user_id favorite_stuff operation metadata A Chocolate Add &amp;hellip; A Chocolate Update &amp;hellip; B Milk Remove &amp;hellip; B Cornflakes Add &amp;hellip; De tal manera que habría que combinar todos los eventos para saber cual es el perfil actual del usuario y aplicar cierta lógica.</description></item><item><title>Calcular el domingo de la semana</title><link>https://adrianabreu.gitlab.io/blog/2020-09-02-calcular-el-domingo-correspondiente/</link><pubDate>Wed, 02 Sep 2020 10:12:32 +0000</pubDate><guid>https://adrianabreu.gitlab.io/blog/2020-09-02-calcular-el-domingo-correspondiente/</guid><description>A la hora de publicar reportes es común agrupar los datos por semanas. Otro motivo es alinearse con el negocio donde los cierres pueden producirse en días concretos, por ejemplo, un domingo.
En esos casos si tenemos los datos particionados por días nos interesa saber a que domingo correspondería cada uno de los datos.
Los que venimos de otros entornos tendemos a pensar en esas complicadas librerías de fechas (moment.js, jodatime, etc).</description></item><item><title>Spark windows functions (I)</title><link>https://adrianabreu.gitlab.io/blog/2020-08-11-spark-windows-functions/</link><pubDate>Tue, 11 Aug 2020 18:52:32 +0000</pubDate><guid>https://adrianabreu.gitlab.io/blog/2020-08-11-spark-windows-functions/</guid><description>En analítica, es muy común hacer uso de las funciones de ventana para distintos cálculos. Hace poco me encontré con un pequeño problema cuya solución mejoró muchísimo al usar las funciones de ventana, demos un poco de contexto.
Tenemos una dimensión de usuarios donde los usuarios se van registrando con una fecha y tenemos una tabla de ventas donde tenemos las ventas globales para cada día
Y lo que queremos dar es una visión de cómo cada día evoluciona el programa, para ello se quiere que cada día estén tanto las ventas acumuladas como los registros acumulados.</description></item><item><title>Spark windows functions (II)</title><link>https://adrianabreu.gitlab.io/blog/2025-01-01-spark-windows-functions-ii/</link><pubDate>Tue, 11 Aug 2020 18:52:32 +0000</pubDate><guid>https://adrianabreu.gitlab.io/blog/2025-01-01-spark-windows-functions-ii/</guid><description>En el post anterior pudimos utilizar las funciones de ventanas para realizar agregados de sumas sobre ventanas temporales. Ahora, me gustaría utilizar otro ejemplo de analítica: Comparar con datos previos.
Pongamos que queremos analizar las ventas de un producto en diversos periodos de tiempo. Es decir, nos interesa saber si ahora vende más o menos que antes.
Para ello partiremos de una tabla de ventas:
DateKey ProductId Sales Ahora que tenemos esto nos interesaría agrupar para cada producto sus ventas</description></item><item><title>Correlated subqueries</title><link>https://adrianabreu.gitlab.io/blog/2019-09-26-correlated-subqueries/</link><pubDate>Thu, 26 Sep 2019 20:43:32 +0000</pubDate><guid>https://adrianabreu.gitlab.io/blog/2019-09-26-correlated-subqueries/</guid><description>Llevo un par de meses viendo como la mayoría de esfuerzos en el proyecto en el que estoy se centran en evitar los joins en las distintas capas de análisis. Aprovechando las capacidades de spark se busca tener las estructuras muy desnormalizadas y se había &amp;ldquo;endemoniado&amp;rdquo; al join considerarlo perjudicial.
Tanto es así que llevo un par de días peleando con una pieza de código que me ha sorprendido. Partiendo de una tabla de hechos que agrupa datos para un periodo a hasta b, se quiere que se &amp;ldquo;colapsen&amp;rdquo; los datos de hace 14 días.</description></item><item><title>Datos I - Introducción al Datawarehousing</title><link>https://adrianabreu.gitlab.io/blog/2019-02-05-data-i-introduccion-al-datawarehouse/</link><pubDate>Tue, 05 Feb 2019 14:43:32 +0000</pubDate><guid>https://adrianabreu.gitlab.io/blog/2019-02-05-data-i-introduccion-al-datawarehouse/</guid><description>En los últimos meses mi trabajo ha pivotado del mundo de la web al mundo de los datos. He entrado a participar en un proyecto de Data Warehouse y he acabado muy contento en él. Hace unos días mi cambio se oficializó completamente y ahora me he dado cuenta de que no solo tengo un mundo técnico ante mí, sino que además necesito consolidar algunas bases teóricas.
Investigando la bibliografía, me han recomendado en Reddit: The Data Warehouse Toolkit, The Complete Guide to Dimensional Modeling 2nd Edition.</description></item></channel></rss>