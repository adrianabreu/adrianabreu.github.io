<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SQL on Adrián Abreu</title><link>https://adrianabreu.com/tags/sql/</link><description>Recent content in SQL on Adrián Abreu</description><generator>Hugo</generator><language>es-ES</language><copyright>2017-2024 Adrián Abreu powered by Hugo and Kiss Theme</copyright><lastBuildDate>Mon, 20 Mar 2023 09:50:32 +0000</lastBuildDate><atom:link href="https://adrianabreu.com/tags/sql/index.xml" rel="self" type="application/rss+xml"/><item><title>Duplicates with delta, how can it be?</title><link>https://adrianabreu.com/blog/2023-03-20-delta-duplicates/</link><pubDate>Mon, 20 Mar 2023 09:50:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2023-03-20-delta-duplicates/</guid><description>&lt;p&gt;Long time without writing!
On highlights: I left my job at &lt;strong&gt;Schwarz It&lt;/strong&gt; in December last year, and now I&amp;rsquo;m a full-time employee at Wallbox! I&amp;rsquo;m really happy with my new job, and I&amp;rsquo;ve experienced interesting stuff. This one was just one of these strange cases where you start doubting the compiler.&lt;/p&gt;
&lt;h2 id="context"&gt;Context&lt;/h2&gt;
&lt;p&gt;One of my main tables represents sensor measures from our chargers with millisecond precision. The numbers are quite high, we are talking over 2 billion rows per day. So the analytic model doesn&amp;rsquo;t handle that level of granularity.
The analyst created a table that will make a window of 5 minutes, select some specific sensors and write there those values as a column. To keep the data consistent they were generating fake rows between sessions, so if a value was missing a synthetic value would be put in place.&lt;/p&gt;</description></item><item><title>A funny bug</title><link>https://adrianabreu.com/blog/2021-07-21-funny-bug/</link><pubDate>Wed, 21 Jul 2021 18:52:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2021-07-21-funny-bug/</guid><description>&lt;p&gt;Ayer mismo estaba intentando analizar unos data_types para completar una exportación al backend. La idea era sencilla, buscar para cada usuario la ultima información disponible en una tabla que representaba actualizaciones sobre su perfil.&lt;/p&gt;
&lt;p&gt;Como a su perfil podías &amp;ldquo;añadir&amp;rdquo;, &amp;ldquo;actualizar&amp;rdquo; y &amp;ldquo;eliminar&amp;rdquo; cosas, pues existian los tres tipos en la tabla. Para que la imaginemos mejor sería tal que así:&lt;/p&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;user_id&lt;/th&gt;
 &lt;th&gt;favorite_stuff&lt;/th&gt;
 &lt;th&gt;operation&lt;/th&gt;
 &lt;th&gt;metadata&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;A&lt;/td&gt;
 &lt;td&gt;Chocolate&lt;/td&gt;
 &lt;td&gt;Add&lt;/td&gt;
 &lt;td&gt;&amp;hellip;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;A&lt;/td&gt;
 &lt;td&gt;Chocolate&lt;/td&gt;
 &lt;td&gt;Update&lt;/td&gt;
 &lt;td&gt;&amp;hellip;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;B&lt;/td&gt;
 &lt;td&gt;Milk&lt;/td&gt;
 &lt;td&gt;Remove&lt;/td&gt;
 &lt;td&gt;&amp;hellip;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;B&lt;/td&gt;
 &lt;td&gt;Cornflakes&lt;/td&gt;
 &lt;td&gt;Add&lt;/td&gt;
 &lt;td&gt;&amp;hellip;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;De tal manera que habría que combinar todos los eventos para saber cual es el perfil actual del usuario y aplicar cierta lógica. Sin embargo los eventos que habían llegado realmente eran así:&lt;/p&gt;</description></item><item><title>Calcular el domingo de la semana</title><link>https://adrianabreu.com/blog/2020-09-02-calcular-el-domingo-correspondiente/</link><pubDate>Wed, 02 Sep 2020 10:12:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2020-09-02-calcular-el-domingo-correspondiente/</guid><description>&lt;p&gt;A la hora de publicar reportes es común agrupar los datos por semanas. Otro motivo es alinearse con el negocio donde los cierres pueden producirse en días concretos, por ejemplo, un domingo.&lt;/p&gt;
&lt;p&gt;En esos casos si tenemos los datos particionados por días nos interesa saber a que domingo correspondería cada uno de los datos.&lt;/p&gt;
&lt;p&gt;Los que venimos de otros entornos tendemos a pensar en esas complicadas librerías de fechas (moment.js, jodatime, etc). Incluso alguien podría pensar en extraer los datos del dataframe y procesarlo en local.&lt;/p&gt;</description></item><item><title>Spark windows functions (I)</title><link>https://adrianabreu.com/blog/2020-08-11-spark-windows-functions/</link><pubDate>Tue, 11 Aug 2020 18:52:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2020-08-11-spark-windows-functions/</guid><description>&lt;p&gt;En analítica, es muy común hacer uso de las funciones de ventana para distintos cálculos. Hace poco me encontré con un pequeño problema cuya solución mejoró muchísimo al usar las funciones de ventana, demos un poco de contexto.&lt;/p&gt;
&lt;p&gt;Tenemos una dimensión de usuarios donde los usuarios se van registrando con una fecha y tenemos una tabla de ventas donde tenemos las ventas globales para cada día&lt;/p&gt;
&lt;p&gt;Y lo que queremos dar es una visión de cómo cada día evoluciona el programa, para ello se quiere que cada día estén tanto las ventas acumuladas como los registros acumulados.&lt;/p&gt;</description></item><item><title>Correlated subqueries</title><link>https://adrianabreu.com/blog/2019-09-26-correlated-subqueries/</link><pubDate>Thu, 26 Sep 2019 20:43:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2019-09-26-correlated-subqueries/</guid><description>&lt;p&gt;Llevo un par de meses viendo como la mayoría de esfuerzos en el proyecto en el que estoy se centran en evitar los joins en las distintas capas de análisis. Aprovechando las capacidades de spark se busca tener las estructuras muy desnormalizadas y se había &amp;ldquo;endemoniado&amp;rdquo; al join considerarlo perjudicial.&lt;/p&gt;
&lt;p&gt;Tanto es así que llevo un par de días peleando con una pieza de código que me ha sorprendido. Partiendo de una tabla de hechos que agrupa datos para un periodo a hasta b, se quiere que se &amp;ldquo;colapsen&amp;rdquo; los datos de hace 14 días. Será mejor con un ejemplo:&lt;/p&gt;</description></item><item><title>Datos I - Introducción al Datawarehousing</title><link>https://adrianabreu.com/blog/2019-02-05-data-i-introduccion-al-datawarehouse/</link><pubDate>Tue, 05 Feb 2019 14:43:32 +0000</pubDate><guid>https://adrianabreu.com/blog/2019-02-05-data-i-introduccion-al-datawarehouse/</guid><description>&lt;p&gt;En los últimos meses mi trabajo ha pivotado del mundo de la web al mundo de los datos. He entrado a participar en un proyecto de Data Warehouse y he acabado muy contento en él. Hace unos días mi cambio se oficializó completamente y ahora me he dado cuenta de que no solo tengo un mundo técnico ante mí, sino que además necesito consolidar algunas bases teóricas.&lt;/p&gt;
&lt;p&gt;Investigando la bibliografía, me han recomendado en Reddit: &lt;em&gt;The Data Warehouse Toolkit, The Complete Guide to Dimensional Modeling 2nd Edition.&lt;/em&gt;
Y el libro parece encajar perfectamente en el conocimiento que busco. Aun así, como todo, por necesidad, intentaré resumir en unas cuantas entradas el conocimiento que se puede obtener de este libro. El cual recomiendo encarecidamente.&lt;/p&gt;</description></item></channel></rss>