<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"  lang="es-ES">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1"/>

<title>Spark Dataframes | Adrián Abreu</title>

<meta property='og:title' content='Spark Dataframes - Adrián Abreu'>
<meta property='og:description' content='Spark was initially released for dealing with a particular type of data called RDD. Nowadays we work with abstract structures on top of it, and the following tables summarize them.
Type Description Advantages Datasets Structured composed of a list of where you can specify your custom class (only Scala) Type-safe operations, support for operations that cannot be expressed otherwise. Dataframes Datasets of type Row (a generic spark type) Allow optimizations and are more flexible SQL tables and views Same as Dataframes but in the scope of databases instead of programming languages Let&rsquo;s dig into the Dataframes.'>
<meta property='og:url' content='https://adrianabreu.gitlab.io/spark-certification/2022-06-10-spark-structured-api/'>
<meta property='og:site_name' content='Adrián Abreu'>
<meta property='og:type' content='article'><meta property='og:image' content='https://www.gravatar.com/avatar/a77b883f262f485b0135c6e352fc6b87?s=256'><meta property='article:section' content='Spark-Certification'><meta property='article:tag' content='Spark'><meta property='article:tag' content='Certification'><meta property='article:tag' content='Data Engineer'><meta property='article:published_time' content='2022-06-10T17:02:32Z'/><meta property='article:modified_time' content='2022-06-10T17:02:32Z'/><meta name='twitter:card' content='summary'><meta name='twitter:site' content='@adrianabreudev'><meta name='twitter:creator' content='@adrianabreudev'>

<link rel="stylesheet" href="/css/style.css"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<link rel="canonical" href="https://adrianabreu.gitlab.io/spark-certification/2022-06-10-spark-structured-api/">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<meta property="og:image" content="/preview.png" />
<meta name="twitter:image" content="/preview.png" />
<script
    defer
    src="https://unpkg.com/@tinybirdco/flock.js"
    data-token="p.eyJ1IjogImUzY2IxNjA4LThmYjEtNDBjNC04NzNlLTFmNjE3ZmI2NGMzMCIsICJpZCI6ICI1NzQ5MDAyMC02Yjg1LTQ2YjEtYWVmOS1lNzMwMjYwYmM1YmQiLCAiaG9zdCI6ICJldV9zaGFyZWQifQ.bcYOUv9CsZ5Fg6UoqxSZk-SYZ6Z_mdrzjbEiE4TPcAA">
</script>
</head>
<body>
<section class="section">
  <div class="container">
    <nav id="nav-main" class="nav">
      <div id="nav-name" class="nav-left">
        <a id="nav-anchor" class="nav-item" href="https://adrianabreu.gitlab.io/">
          <h1 id="nav-heading" class="title is-4">Adrián Abreu</h1>
        </a>
      </div>
      <div class="nav-right">
        <nav id="nav-items" class="nav-item level is-mobile"><a class="level-item" aria-label="github" href='https:/github.com/adrianabreu'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" aria-label="twitter" href='https://twitter.com/adrianabreudev'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" aria-label="email" href='mailto:adrianabreugonzalez@outlook.com'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"/>
    <polyline points="22,6 12,13 2,6"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" aria-label="linkedin" href='https://linkedin.com/in/AdrianAbreu'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path stroke-width="1.8" d="m5.839218,4.101561c0,1.211972 -0.974141,2.194011 -2.176459,2.194011s-2.176459,-0.982039 -2.176459,-2.194011c0,-1.211094 0.974141,-2.194011 2.176459,-2.194011s2.176459,0.982917 2.176459,2.194011zm0.017552,3.94922l-4.388022,0l0,14.04167l4.388022,0l0,-14.04167zm7.005038,0l-4.359939,0l0,14.04167l4.360816,0l0,-7.370999c0,-4.098413 5.291077,-4.433657 5.291077,0l0,7.370999l4.377491,0l0,-8.89101c0,-6.915523 -7.829986,-6.66365 -9.669445,-3.259423l0,-1.891237z"/>
    
  </svg></i>
            </span>
          </a></nav>
      </div>
    </nav>

    <nav class="nav">
      

      
    </nav>

  </div>
  <script src="/js/navicon-shift.js"></script>
</section>
<section class="section">
  <div class="container">
    <div class="subtitle tags is-6 is-pulled-right">
      
      
<a class="subtitle is-6" href="/tags/spark/">#Spark</a>



  
  | <a class="subtitle is-6" href="/tags/certification/">#Certification</a>
  
  | <a class="subtitle is-6" href="/tags/data-engineer/">#Data Engineer</a>
  


      
    </div>
    <h2 class="subtitle is-6">June 10, 2022</h2>
    <h1 class="title">Spark Dataframes</h1>
    
    <div class="content">
      <p>Spark was initially released for dealing with a particular type of data called <strong>RDD</strong>. Nowadays we work with abstract structures on top of it, and the following tables summarize them.</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
<th>Advantages</th>
</tr>
</thead>
<tbody>
<tr>
<td>Datasets</td>
<td>Structured composed of a list of <T> where you can specify your custom class (only Scala)</td>
<td>Type-safe operations, support for operations that cannot be expressed otherwise.</td>
</tr>
<tr>
<td>Dataframes</td>
<td>Datasets of type Row (a generic spark type)</td>
<td>Allow optimizations and are more flexible</td>
</tr>
<tr>
<td>SQL tables and views</td>
<td>Same as Dataframes but in the scope of databases instead of programming languages</td>
<td></td>
</tr>
</tbody>
</table>
<p>Let&rsquo;s dig into the Dataframes.
They are a data abstraction for interacting with name columns, those names are defined in a <strong>schema</strong>.</p>
<p>We have several ways of creating Dataframes through a <em>Spark Session</em>, which can be built through the builder.</p>
<pre tabindex="0"><code>from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName(&#34;Python Spark SQL basic example&#34;) \
    .config(&#34;spark.some.config.option&#34;, &#34;some-value&#34;) \
    .getOrCreate()
</code></pre><table>
<thead>
<tr>
<th>Method</th>
<th>Scope</th>
</tr>
</thead>
<tbody>
<tr>
<td>SQL</td>
<td>Queries</td>
</tr>
<tr>
<td>table</td>
<td>Metastore tables</td>
</tr>
<tr>
<td>read</td>
<td>Files</td>
</tr>
<tr>
<td>range</td>
<td></td>
</tr>
<tr>
<td>createDataframe</td>
<td>Testing</td>
</tr>
</tbody>
</table>
<p>While reading from files there are several data sources: CSV, Parquet, JSON, Delta&hellip; And each one has its particular option. For example, for the CSV you can point out if the header is included or what is the field delimiter.</p>
<p>For those sources, the schema can be inferred or given. We can declare and schema programmatically using the <strong>StructType</strong> methods in the package <strong>pyspark.sql.types</strong>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.types <span style="color:#f92672">import</span> (StructType, StructField, StringType, LongType)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>schema <span style="color:#f92672">=</span> StructType([
</span></span><span style="display:flex;"><span>    StructField(<span style="color:#e6db74">&#34;user_id&#34;</span>, StringType()),
</span></span><span style="display:flex;"><span>    StructField(<span style="color:#e6db74">&#34;user_first_touch_timestamp&#34;</span>, LongType()),
</span></span><span style="display:flex;"><span>    StructField(<span style="color:#e6db74">&#34;email&#34;</span>, StringType())
</span></span><span style="display:flex;"><span>])
</span></span></code></pre></div><p>Also, we can define the same schema using the DDL syntax</p>
<pre tabindex="0"><code>&#34;user_id string, user_first_touch_timestamp long, email sting&#34;
</code></pre><p>When we get a dataframe from a source we can apply several transformations to it, we can get rows that match a given condition (filter), select a subset of columns (select), add a new column (withColumn), or maybe group rows by some columns (groupBy). Any of them are the lazy-evaluate expressions we talked about in the spark execution and all the methods and signatures are available in the <a href="https://spark.apache.org/docs/latest/api/python/reference/index.html">official docs</a>.</p>
<p>The last transformation mentioned (groupBy) is quite special since is the only one that cannot be chained to others. While you can write:</p>
<pre tabindex="0"><code>df.withColumn().drop().filter().select().filter().drop().withColumn ...
</code></pre><p>The groupBy will require shuffle and will allow for aggregations to be run on top of it so the result of a groupBy is not a Dataframe but a <strong>RelationalGroupDataset</strong></p>
<p>To use get back to a dataframe we need to run the <code>agg</code> method. Then we are calculating a single value on a group of rows. Users can create their custom aggregate functions alias <strong>udaf</strong>.</p>
<p>When we use those methods we are indexing the values by columns, those are just a representation of a value computed for each record. They can be accessed in different ways:</p>
<pre tabindex="0"><code>df(&#34;columnName&#34;)
col(&#34;colName&#34;)
$&#34;columnName&#34;
$&#34;columnName.field&#34;
</code></pre><p>And provide an API for applying conditions on top of them for filtering, for example, here is a list of operators for columns:</p>
<table>
<thead>
<tr>
<th>Operator</th>
<th>Method</th>
</tr>
</thead>
<tbody>
<tr>
<td>&amp;&amp;,</td>
<td></td>
</tr>
<tr>
<td>*, +, &lt;, ≥</td>
<td>Math and comparison</td>
</tr>
<tr>
<td>alias, as</td>
<td>alias of columns</td>
</tr>
<tr>
<td>cast</td>
<td>cast to different data_type</td>
</tr>
<tr>
<td>isNull, isNan</td>
<td>check null equality</td>
</tr>
</tbody>
</table>

      
    </div>
    
  </div>
</section>



<section class="section">
  <div class="container has-text-centered">
    <p>2017-2022 Adrián Abreu powered by Hugo and Kiss Theme</p>
    
  </div>
</section>



</body>
</html>

