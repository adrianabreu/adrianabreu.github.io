<!doctype html><html xmlns=http://www.w3.org/1999/xhtml lang=es-es><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Spark DataSources | Adrián Abreu</title><meta property="og:title" content="Spark DataSources - Adrián Abreu"><meta property="og:description" content="As estated in the structured api section, Spark supports a lot of sources with a lot of options. There is no other goal for this post than to clarify how the most common ones work and how they will be converted to DataFrames.
First, all the supported sources are listed here: https://spark.apache.org/docs/latest/sql-data-sources.html
And we can focus on the typical ones: JSON, CSV and Parquet (as those are the typical format on open-source data)."><meta property="og:url" content="https://adrianabreu.github.io/spark-certification/2022-06-11-spark-data-sources/"><meta property="og:site_name" content="Adrián Abreu"><meta property="og:type" content="article"><meta property="og:image" content="https://www.gravatar.com/avatar/9fda37f7195de6954a6d4f525eff01ee?s=256"><meta property="article:section" content="Spark-Certification"><meta property="article:tag" content="Spark"><meta property="article:tag" content="Certification"><meta property="article:tag" content="Data Engineer"><meta property="article:published_time" content="2022-06-11T16:43:22Z"><meta property="article:modified_time" content="2022-06-11T16:43:22Z"><meta name=twitter:card content="summary"><meta name=twitter:site content="@aabreuglez"><meta name=twitter:creator content="@aabreuglez"><link href=https://adrianabreu.github.io/index.xml rel=alternate type=application/rss+xml title="Adrián Abreu"><link rel=stylesheet href=/css/style.css><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical href=https://adrianabreu.github.io/spark-certification/2022-06-11-spark-data-sources/><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"></head><body><section class=section><div class=container><nav id=nav-main class=nav><div id=nav-name class=nav-left><a id=nav-anchor class=nav-item href=https://adrianabreu.github.io><h1 id=nav-heading class="title is-4">Adrián Abreu</h1></a></div><div class=nav-right><nav id=nav-items class="nav-item level is-mobile"><a class=level-item aria-label=github href=https://github.com/adrianabreu target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></i></span></a><a class=level-item aria-label=twitter href=https://twitter.com/aabreuglez target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg></i></span></a><a class=level-item aria-label=email href=mailto:aabreuglez@gmail.com target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></i></span></a><a class=level-item aria-label=linkedin href=https://linkedin.com/in/AdrianAbreu target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path stroke-width="1.8" d="m5.839218 4.101561c0 1.211972-.974141 2.194011-2.176459 2.194011S1.4863 5.313533 1.4863 4.101561c0-1.211094.974141-2.194011 2.176459-2.194011s2.176459.982917 2.176459 2.194011zm.017552 3.94922H1.468748v14.04167H5.85677V8.050781zm7.005038.0H8.501869v14.04167h4.360816v-7.370999c0-4.098413 5.291077-4.433657 5.291077.0v7.370999h4.377491v-8.89101c0-6.915523-7.829986-6.66365-9.669445-3.259423V8.050781z"/></svg></i></span></a></nav></div></nav><nav class=nav></nav></div><script src=/js/navicon-shift.js></script></section><section class=section><div class=container><div class="subtitle tags is-6 is-pulled-right"><a class="subtitle is-6" href=/tags/spark/>#Spark</a>
| <a class="subtitle is-6" href=/tags/certification/>#Certification</a>
| <a class="subtitle is-6" href=/tags/data-engineer/>#Data Engineer</a></div><h2 class="subtitle is-6">June 11, 2022</h2><h1 class=title>Spark DataSources</h1><div class=content><p>As estated in the <a href=/spark-certification/2022-06-10-spark-structured-api>structured api section</a>, Spark supports a lot of sources with a lot of options. There is no other goal for this post than to clarify how the most common ones work and how they will be converted to <strong>DataFrames</strong>.</p><p>First, all the supported sources are listed here: <a href=https://spark.apache.org/docs/latest/sql-data-sources.html>https://spark.apache.org/docs/latest/sql-data-sources.html</a></p><p>And we can focus on the typical ones: JSON, CSV and Parquet (as those are the typical format on open-source data).</p><p>There are two main options available to almost all datasources, the mode and the schema. This mode performs different functions if it for read or write. For reading it allow us to choose an strategy for dealing with malformed data. We could abort the read (FAILFAST), ignore them or put them under some specific path. And for writing it allow us to specify what to do when there is data in the destination path: Overwrite all data? Append Only?.</p><p>The schema allow us to specify before-hand how the data should be. And while we can infer the schema from the files, there is a cost on it, let&rsquo;s review it.</p><p>I&rsquo;m using <a href=https://community.cloud.databricks.com/?>Databricks Community</a> with the sample dataset <code>dbfs:/databricks-datasets/amazon/users/part-r-00000-f8d9888b-ba9e-47bb-9501-a877f2574b3c.csv</code>.</p><p>First, let&rsquo;s read the file without specifying the schema:</p><pre tabindex=0><code>spark.read.option(&#34;header&#34;,&#34;true&#34;).csv(&#34;dbfs:/databricks-datasets/amazon/users/part-r-00000-f8d9888b-ba9e-47bb-9501-a877f2574b3c.csv&#34;)

(1) Spark Jobs
res0: org.apache.spark.sql.DataFrame = [user: string]
Command took 6.74 seconds
</code></pre><p>As we can see there is a Spark Job triggered, why? Because we need to infer the schema of the file. So there is a job for the driver to infer the schema of the files list. If we get into the execution plan it shows it clearly:</p><pre tabindex=0><code>== Physical Plan ==
CollectLimit (3)
+- * Filter (2)
   +- Scan text  (1)


(1) Scan text 
Output [1]: [value#433]
Batched: false
Location: InMemoryFileIndex [dbfs:/databricks-datasets/amazon/users/part-r-00000-f8d9888b-ba9e-47bb-9501-a877f2574b3c.csv]
ReadSchema: struct&lt;value:string&gt;

(2) Filter [codegen id : 1]
Input [1]: [value#433]
Condition : (length(trim(value#433, None)) &gt; 0)

(3) CollectLimit
Input [1]: [value#433]
Arguments: 1
</code></pre><p>On the other hand if we specify the schema there is no job generated:</p><pre tabindex=0><code>from pyspark.sql.types import (StructType, StructField, StringType)

schema = StructType([
  StructField(&#34;user&#34;, StringType())
])

spark.read.schema(schema).option(&#34;header&#34;,&#34;true&#34;).csv(&#34;dbfs:/databricks-datasets/amazon/users/part-r-00000-f8d9888b-ba9e-47bb-9501-a877f2574b3c.csv&#34;)

Out[2]: DataFrame[user: string]
Command took 0.52 seconds 
</code></pre><p>Also if there was a folder it would be extra jobs for listing all the files, checking the existence and it would have to read them all for generating a common schema. The overhead seems small but if you specify a folder with CSV or Jsons, the time can increase exponentially.</p><p>When we read a folder, each file will be a partition and it will be processed by an executor. We cannot read the same file with multiple executors at the same time but we could avoid reading the files entirely if the file is &ldquo;splittable&rdquo; (like Parquet files that are delimited in chunks).</p><p>As 1 file matches 1 partition, when we write 1 partition will be 1 file. If we write the data directly we will end up with a lot of files that will be impact the performance of another job reading them. (For each file there is the overhead we mentioned before). We will need to balance the partitions using repartition or coalesce in our dataframe.</p><p>Also there are two write options for it:</p><ol><li>partitionBy: allow us to order the files in folder by the given columns. Those folders will respect the hive style format <code>/field=value/</code>. And they will represent the value for all the files internally. That will allow to skip a lot of data when looking for certain values (typically there is a date_key value).</li><li>bucketing: this will make data with the same bucket_id value to be on the same physical partition. For example for one given day all the sales of each store could be together so the readers will avoid shuffling when they want to perform computations on it. If we partitioned each days sales by store we may end up with a lof of directories that would cause overhead on the storage systems.</li></ol></div></div></section><section class=section><div class="container has-text-centered"><p>2017 Adrián Abreu powered by Hugo and Kiss Theme</p></div></section></body></html>