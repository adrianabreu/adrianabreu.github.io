<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Spark-Certifications on Adrián Abreu</title><link>https://adrianabreu.github.com/spark-certification/</link><description>Recent content in Spark-Certifications on Adrián Abreu</description><generator>Hugo -- gohugo.io</generator><language>es-ES</language><copyright>2017-2022 Adrián Abreu powered by Hugo and Kiss Theme</copyright><lastBuildDate>Thu, 21 Jul 2022 12:16:32 +0000</lastBuildDate><atom:link href="https://adrianabreu.github.com/spark-certification/index.xml" rel="self" type="application/rss+xml"/><item><title>Associate Spark Developer Certification</title><link>https://adrianabreu.github.com/spark-certification/2022-07-21-passed-certification/</link><pubDate>Thu, 21 Jul 2022 12:16:32 +0000</pubDate><guid>https://adrianabreu.github.com/spark-certification/2022-07-21-passed-certification/</guid><description>Yesterday I took (and passed with more than 90% yay!) the Associate Spark Developer Certificaton. And before I forget I want to share my experience:
In general:
First of all, I needed to install Windows as there was no Linux support for the control software used during the exam. Secondly, you need to disable both the antivirus and the firewall before joining. I didn&amp;rsquo;t disable the antivirus and the technician contacted me as there was a problem with the webcam despite I was able to see myself.</description></item><item><title>Spark Dates</title><link>https://adrianabreu.github.com/spark-certification/2022-06-29-spark-dates/</link><pubDate>Wed, 29 Jun 2022 15:43:22 +0000</pubDate><guid>https://adrianabreu.github.com/spark-certification/2022-06-29-spark-dates/</guid><description>I can perfectly describe this as the scariest part of the exam. I&amp;rsquo;m used to working with dates but I&amp;rsquo;m especially used to suffering from the typical UTC / not UTC / summer time hours difference.
I will try to make some simple exercises for this, the idea would be:
We have some sales data and god knows how the business people love to refresh super fast their dashboards on Databricks SQL.</description></item><item><title>Spark Cert Exam Practice</title><link>https://adrianabreu.github.com/spark-certification/2022-06-28-databricks-practice-exam/</link><pubDate>Tue, 28 Jun 2022 13:43:22 +0000</pubDate><guid>https://adrianabreu.github.com/spark-certification/2022-06-28-databricks-practice-exam/</guid><description>--- primary_color: orange secondary_color: lightgray text_color: black shuffle_questions: false --- ## Which of the following statements about the Spark driver is incorrect? - [ ] The Spark driver is the node in which the Spark application's main method runs to ordinate the Spark application. - [X] The Spark driver is horizontally scaled to increase overall processing throughput. - [ ] The Spark driver contains the SparkContext object. - [ ] The Spark driver is responsible for scheduling the execution of data by various worker nodes in cluster mode.</description></item><item><title>Spark User Defined Functions</title><link>https://adrianabreu.github.com/spark-certification/2022-06-19-spark-udf-udaf/</link><pubDate>Sun, 19 Jun 2022 14:43:22 +0000</pubDate><guid>https://adrianabreu.github.com/spark-certification/2022-06-19-spark-udf-udaf/</guid><description>Sometimes we need to execute arbitrary Scala code on Spark. We may need to use an external library or so on. For that, we have the UDF, which accepts and return one or more columns.
When we have a function we need to register it on Spark so we can use it on our worker machines. If you are using Scala or Java, the udf can run inside the Java Virtual Machine so there&amp;rsquo;s a little extra penalty.</description></item><item><title>Spark DataSources</title><link>https://adrianabreu.github.com/spark-certification/2022-06-11-spark-data-sources/</link><pubDate>Sat, 11 Jun 2022 16:43:22 +0000</pubDate><guid>https://adrianabreu.github.com/spark-certification/2022-06-11-spark-data-sources/</guid><description>As estated in the structured api section, Spark supports a lot of sources with a lot of options. There is no other goal for this post than to clarify how the most common ones work and how they will be converted to DataFrames.
First, all the supported sources are listed here: https://spark.apache.org/docs/latest/sql-data-sources.html
And we can focus on the typical ones: JSON, CSV and Parquet (as those are the typical format on open-source data).</description></item><item><title>Spark Dataframes</title><link>https://adrianabreu.github.com/spark-certification/2022-06-10-spark-structured-api/</link><pubDate>Fri, 10 Jun 2022 17:02:32 +0000</pubDate><guid>https://adrianabreu.github.com/spark-certification/2022-06-10-spark-structured-api/</guid><description>Spark was initially released for dealing with a particular type of data called RDD. Nowadays we work with abstract structures on top of it, and the following tables summarize them.
Type Description Advantages Datasets Structured composed of a list of where you can specify your custom class (only Scala) Type-safe operations, support for operations that cannot be expressed otherwise. Dataframes Datasets of type Row (a generic spark type) Allow optimizations and are more flexible SQL tables and views Same as Dataframes but in the scope of databases instead of programming languages Let&amp;rsquo;s dig into the Dataframes.</description></item><item><title>Spark Execution</title><link>https://adrianabreu.github.com/spark-certification/2022-06-08-spark-execution/</link><pubDate>Wed, 08 Jun 2022 17:02:32 +0000</pubDate><guid>https://adrianabreu.github.com/spark-certification/2022-06-08-spark-execution/</guid><description>Spark provides an api and an engine, that engine is responsible for analyzing the code and performing several optimizations. But how does this work? We can do two kinds of operations with Spark, transformations and actions.
Transformations are operations on top of the data that modify the data but do not yield a result directly, that is because they all are lazily evaluated so, you can add new columns, filter rows, or perform some computations that won&amp;rsquo;t be executed immediately.</description></item><item><title>Spark Architecture</title><link>https://adrianabreu.github.com/spark-certification/2022-06-07-spark-architecture/</link><pubDate>Tue, 07 Jun 2022 17:02:32 +0000</pubDate><guid>https://adrianabreu.github.com/spark-certification/2022-06-07-spark-architecture/</guid><description>Spark works on top of a cluster supervised by a cluster manager. The later is responsible of:
Tracking resource allocation across all applications running on the cluster. Monitoring the health of all the nodes. Inside each node there is a node manager which is responsible to track each node health and resources and inform the cluster manager.
C l u s t e r M a n a g e r N N N o o o d d d e e e M M M a a a n n n a a a g g g e e e r r r When we run a Spark application we generate processes inside the cluster where one node will act as a Driver and the rest will be Workers.</description></item></channel></rss>