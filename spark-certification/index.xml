<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Spark-certifications on Adrián Abreu</title><link>https://adrianabreu.github.io/spark-certification/</link><description>Recent content in Spark-certifications on Adrián Abreu</description><generator>Hugo -- gohugo.io</generator><language>es-ES</language><copyright>2017 Adrián Abreu powered by Hugo and Kiss Theme</copyright><lastBuildDate>Sat, 11 Jun 2022 16:43:22 +0000</lastBuildDate><atom:link href="https://adrianabreu.github.io/spark-certification/index.xml" rel="self" type="application/rss+xml"/><item><title>Spark DataSources</title><link>https://adrianabreu.github.io/spark-certification/2022-06-11-spark-data-sources/</link><pubDate>Sat, 11 Jun 2022 16:43:22 +0000</pubDate><guid>https://adrianabreu.github.io/spark-certification/2022-06-11-spark-data-sources/</guid><description>As estated in the structured api section, Spark supports a lot of sources with a lot of options. There is no other goal for this post than to clarify how the most common ones work and how they will be converted to DataFrames.
First, all the supported sources are listed here: https://spark.apache.org/docs/latest/sql-data-sources.html
And we can focus on the typical ones: JSON, CSV and Parquet (as those are the typical format on open-source data).</description></item><item><title>Spark Dataframes</title><link>https://adrianabreu.github.io/spark-certification/2022-06-10-spark-structured-api/</link><pubDate>Fri, 10 Jun 2022 17:02:32 +0000</pubDate><guid>https://adrianabreu.github.io/spark-certification/2022-06-10-spark-structured-api/</guid><description>Spark was initially released for dealing with a particular type of data called RDD. Nowadays we work with abstract structures on top of it, and the following tables summarize them.
Type Description Advantages Datasets Structured composed of a list of where you can specify your custom class (only Scala) Type-safe operations, support for operations that cannot be expressed otherwise. Dataframes Datasets of type Row (a generic spark type) Allow optimizations and are more flexible SQL tables and views Same as Dataframes but in the scope of databases instead of programming languages Let&amp;rsquo;s dig into the Dataframes.</description></item><item><title>Spark Execution</title><link>https://adrianabreu.github.io/spark-certification/2022-06-08-spark-execution/</link><pubDate>Wed, 08 Jun 2022 17:02:32 +0000</pubDate><guid>https://adrianabreu.github.io/spark-certification/2022-06-08-spark-execution/</guid><description>Spark provides an api and an engine, that engine is responsible for analyzing the code and performing several optimizations. But how does this work? We can do two kinds of operations with Spark, transformations and actions.
Transformations are operations on top of the data that modify the data but do not yield a result directly, that is because they all are lazily evaluated so, you can add new columns, filter rows, or perform some computations that won&amp;rsquo;t be executed immediately.</description></item><item><title>Spark Architecture</title><link>https://adrianabreu.github.io/spark-certification/2022-06-07-spark-architecture/</link><pubDate>Tue, 07 Jun 2022 17:02:32 +0000</pubDate><guid>https://adrianabreu.github.io/spark-certification/2022-06-07-spark-architecture/</guid><description>Spark works on top of a cluster that will be managed by a cluster manager. One node will act as a Driver and the rest will be Workers.
( 1 S ) D p r a i r v k e r W o r k ( e N r ) s Who are that Driver and those Workers? That depends on the Execution Mode.
There 3 kinds:
Cluster Mode: This is the most common way.</description></item></channel></rss>