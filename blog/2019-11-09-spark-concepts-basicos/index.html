<!doctype html><html xmlns=http://www.w3.org/1999/xhtml lang=es-es><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Conceptos básicos de Spark | Adrián Abreu</title><meta property="og:title" content="Conceptos básicos de Spark - Adrián Abreu"><meta property="og:description" content="Nota del autor: Todos los contenidos de este artículo son extractos del libro &ldquo;The Data Engineer&rsquo;s Guide to Apache Spark&rdquo; que puedes descargar desde la pagina de databricks: https://databricks.com/lp/ebook/data-engineer-spark-guide
Preludio: Cluster: Un cluster no es más que un conjunto de máquinas trabajando de forma coordinada. Un cluster de Spark se compone de nodos. Uno actúa como DRIVER y es el punto de entrada para el código del usuario. Los otros actúan como EXECUTOR que seran los encargados de realizar las operaciones."><meta property="og:url" content="https://adrianabreu.github.io/blog/2019-11-09-spark-concepts-basicos/"><meta property="og:site_name" content="Adrián Abreu"><meta property="og:type" content="article"><meta property="og:image" content="https://www.gravatar.com/avatar/9fda37f7195de6954a6d4f525eff01ee?s=256"><meta property="article:section" content="Blog"><meta property="article:tag" content="Spark"><meta property="article:published_time" content="2019-11-09T19:22:32Z"><meta property="article:modified_time" content="2019-11-09T19:22:32Z"><meta name=twitter:card content="summary"><meta name=twitter:site content="@aabreuglez"><meta name=twitter:creator content="@aabreuglez"><link href=https://adrianabreu.github.io/index.xml rel=alternate type=application/rss+xml title="Adrián Abreu"><link rel=stylesheet href=/css/style.css><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical href=https://adrianabreu.github.io/blog/2019-11-09-spark-concepts-basicos/><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"></head><body><section class=section><div class=container><nav id=nav-main class=nav><div id=nav-name class=nav-left><a id=nav-anchor class=nav-item href=https://adrianabreu.github.io><h1 id=nav-heading class="title is-4">Adrián Abreu</h1></a></div><div class=nav-right><nav id=nav-items class="nav-item level is-mobile"><a class=level-item aria-label=github href=https://github.com/adrianabreu target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></i></span></a><a class=level-item aria-label=twitter href=https://twitter.com/aabreuglez target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg></i></span></a><a class=level-item aria-label=email href=mailto:aabreuglez@gmail.com target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></i></span></a><a class=level-item aria-label=linkedin href=https://linkedin.com/in/AdrianAbreu target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path stroke-width="1.8" d="m5.839218 4.101561c0 1.211972-.974141 2.194011-2.176459 2.194011S1.4863 5.313533 1.4863 4.101561c0-1.211094.974141-2.194011 2.176459-2.194011s2.176459.982917 2.176459 2.194011zm.017552 3.94922H1.468748v14.04167H5.85677V8.050781zm7.005038.0H8.501869v14.04167h4.360816v-7.370999c0-4.098413 5.291077-4.433657 5.291077.0v7.370999h4.377491v-8.89101c0-6.915523-7.829986-6.66365-9.669445-3.259423V8.050781z"/></svg></i></span></a></nav></div></nav><nav class=nav></nav></div><script src=/js/navicon-shift.js></script></section><section class=section><div class=container><div class="subtitle tags is-6 is-pulled-right"><a class="subtitle is-6" href=/tags/spark/>#Spark</a></div><h2 class="subtitle is-6">November 9, 2019</h2><h1 class=title>Conceptos básicos de Spark</h1><div class=content><p><strong>Nota del autor</strong>: Todos los contenidos de este artículo son extractos del libro &ldquo;The Data Engineer&rsquo;s Guide to Apache Spark&rdquo; que puedes descargar desde la pagina de databricks: <a href=https://databricks.com/lp/ebook/data-engineer-spark-guide>https://databricks.com/lp/ebook/data-engineer-spark-guide</a></p><h2 id=preludio>Preludio:</h2><h3 id=cluster>Cluster:</h3><p>Un cluster no es más que un conjunto de máquinas trabajando de forma coordinada. Un cluster de Spark se compone de nodos. Uno actúa como DRIVER y es el punto de entrada para el código del usuario. Los otros actúan como EXECUTOR que seran los encargados de realizar las operaciones.</p><h3 id=spark-session>Spark Session:</h3><p>Es el objeto que tenemos para controlar nuestra aplicación de Spark. En Scala la variable &ldquo;spark&rdquo; representa este valor. Podemos generar esta sesión de SparkSession a traves de un &ldquo;builder&rdquo;. (Aquí podremos inyectar configuraciones, etcétera).</p><p><strong>Nota</strong>: Spark Session apareció como estándar en Spark 2.0, antes de esto se utilizaba un &ldquo;Spark Context&rdquo; que nos permitía solamente crear RDD&rsquo;s. Si queríamos hacer algo con sql había que instanciar un sqlContext, para hive un hiveContext&mldr; Para más información, este artículo lo describe muy bien: <a href=https://medium.com/@achilleus/spark-session-10d0d66d1d24>https://medium.com/@achilleus/spark-session-10d0d66d1d24</a></p><h2 id=conceptos-de-tipos-de-datos>Conceptos de tipos de datos:</h2><h3 id=dataframe>DataFrame:</h3><p>Es una representación de una tabla con filas y columnas. Podemos encontrar las columnas que contiene un dataframe y el tipo de las mismas en la propiedad schema. La ventaja de un dataframe es que puede estar distribuido a lo largo de decenas de máquinas. Es la forma más sencilla de trabajar con datos en Spark y tiene la ventaja de contar con un optimizador (Catalyst).</p><h3 id=datasets>Datasets:</h3><p>Son otra representación de datos pero tipados. Como &ldquo;truco&rdquo; un dataframe es un dataset de un tipo abstracto llamado &ldquo;Row&rdquo;.</p><h3 id=rdd>RDD:</h3><p>Son la primera aproximación que se hizo para computar datos de forma distribuida. De hecho se puede acceder al rdd de un dataframe. Se desrecomienda su uso por su &ldquo;fragilidad&rdquo; y porque la api de DataFrame ya ha igualado a la de RDD.</p><h2 id=conceptos-de-operaciones>Conceptos de operaciones:</h2><h3 id=particiones>Particiones:</h3><p>A diferencia de una partición de Hive (una carpeta con datos que podemos &ldquo;esquivar&rdquo; al hacer consultas) estas particiones son trozos de datos. Una serie de filas que residen en la misma máquina fisica. Para ajustar el numero de particiones disponemos de los métodos repartition y coalesce. La información sobre el impacto que tiene esto puede ver en Conceptos de Ejecución -> Tareas.</p><h3 id=transformaciones>Transformaciones:</h3><p>Son las instrucciones que podemos utilizar para modificar un dataframe. Una transformación no da ningún resultado ya que se evalúa mediante &ldquo;lazy evaluation&rdquo;, es decir, puedes encadenar operaciones que hasta que no ocurra una &ldquo;acción&rdquo;, estas transformaciones no se computan. De esta forma, Spark es capaz de organizar las transformaciones para dar el mejor plan de ejecución físico posible y ahorrar la mayor cantidad de &ldquo;shuffling&rdquo; (intercambiar datos entre nodos).</p><h3 id=acciones>Acciones:</h3><p>Son las instrucciones que hacen que se computen las transformaciones previas. La más simple puede ser un count. Pero son acciones cosas del tipo: un show para mostrar los datos, un write para escribir un output, un collect para devolver los datos al nodo driver.</p><h2 id=conceptos-de-ejecución>Conceptos de ejecución:</h2><h3 id=jobs>Jobs:</h3><p>Una acción desencadena un job para computar las operaciones. No es mas que una unidad de organización lógica.</p><h3 id=stages>Stages:</h3><p>Son &ldquo;pasos&rdquo; en una ejecución, basicamente agrupan tareas y resolven dependencias entre sí.</p><h3 id=tasks>Tasks:</h3><p>Es la unidad de ejecución de cualquier worker y la que trabaja con los datos directamente haciendo uso de los recursos. Esta directamente relacionada con el concepto de partición.</p><h2 id=extra>Extra</h2><p>En la web de <a href=https://supergloo.com/spark/spark-fair-scheduler-example/>https://supergloo.com/spark/spark-fair-scheduler-example/</a> se representa perfectamente la jerarquía entre operaciones y ejecución.</p><p><img src=https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_405,h_303/https://supergloo.com/wp-content/uploads/2017/09/spark-fair-scheduler.jpg alt></p></div></div></section><section class=section><div class="container has-text-centered"><p>2017 Adrián Abreu powered by Hugo and Kiss Theme</p></div></section></body></html>