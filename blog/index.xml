<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blogs on Adrián Abreu</title><link>https://adrianabreu.github.io/blog/</link><description>Recent content in Blogs on Adrián Abreu</description><generator>Hugo -- gohugo.io</generator><language>es-ES</language><copyright>2017 Adrián Abreu powered by Hugo and Kiss Theme</copyright><lastBuildDate>Sat, 30 Jul 2022 13:52:32 +0000</lastBuildDate><atom:link href="https://adrianabreu.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Databricks Cluster Management</title><link>https://adrianabreu.github.io/blog/2022-07-30-databricks-cluster-management/</link><pubDate>Sat, 30 Jul 2022 13:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2022-07-30-databricks-cluster-management/</guid><description>For the last few months, I&amp;rsquo;ve been into ETL optimization. Most of the changes were as dramatic as moving tables from ORC to delta revamping the partition strategy to some as simple as upgrading the runtime version to 10.4 so the ETL starts using low-shuffle merge.
But at my job, we have a lot of jobs. Each ETL can be easily launched at *30 with different parameters so I wanted to dig into the most effective strategy for it.</description></item><item><title>Pusing data to tinybird for free</title><link>https://adrianabreu.github.io/blog/2022-07-25-pushing-data-to-tinybird-free/</link><pubDate>Mon, 25 Jul 2022 07:28:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2022-07-25-pushing-data-to-tinybird-free/</guid><description>So my azure subscription expired and I ended up losing the function I was using to feed my real-time data on analytics (part of the Transportes Insulares de Tenerife SA analysis I was making).
And after some struggle, I decided to move it to a GitHub action. Why? Because the free mins per month were more than enough and because I just needed some script to run on a cron and that script just makes a quest and a post.</description></item><item><title>Reading firebase data</title><link>https://adrianabreu.github.io/blog/2022-07-01-reading-firebase-data/</link><pubDate>Fri, 01 Jul 2022 07:28:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2022-07-01-reading-firebase-data/</guid><description>Firebase is a common component nowadays for most mobile apps. And it can provide some useful insights, for example in my previous company we use it to detect where the people left at the initial app wizard. (We could measure it).
It is quite simple to export your data to BigQuery: https://firebase.google.com/docs/projects/bigquery-export
But maybe your lake is in AWS or Azure. In the next lines, I will try to explain how to load the data in your lake and some improvements we have applied.</description></item><item><title>Qbeast</title><link>https://adrianabreu.github.io/blog/2022-06-30-qbeast/</link><pubDate>Thu, 30 Jun 2022 07:28:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2022-06-30-qbeast/</guid><description>A few days ago I ran into Qbeast which is an open-source project on top of delta lake I needed to dig into.
This introductory post explains it quite well: https://qbeast.io/qbeast-format-enhanced-data-lakehouse/
The project is quite good and it seems helpful if you need to write your custom data source as everything is documented. And well as I&amp;rsquo;m in love with note-taking I want to dig into the following three topics:</description></item><item><title>Faker with PySpark</title><link>https://adrianabreu.github.io/blog/2022-05-31-faker-pyspark/</link><pubDate>Tue, 31 May 2022 09:28:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2022-05-31-faker-pyspark/</guid><description>I’m preparing a small blog post about some tweakings I’ve done for a delta table, but I want to dig into the Spark UI differences before this. As this was done as part of my work I’m reproducing the problem with some generated data.
I didn’t know about Faker and boy it is really simple and easy.
In this case, I want to generate a small dataset for a dimension product table including its id, category and price.</description></item><item><title>Git 101</title><link>https://adrianabreu.github.io/blog/2022-03-21-git-intro/</link><pubDate>Mon, 21 Mar 2022 22:28:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2022-03-21-git-intro/</guid><description>From time to time I get to the same place, telling some people about git, what it solves and some basic usage.
Since I&amp;rsquo;ve done it a lot recenly I wanted to write down a post and enjoy it.
What is git? Git is a gift from the gods for the following use cases:
My laptop is broke! I need the data there is a whole month of work there!</description></item><item><title>Sbt tests</title><link>https://adrianabreu.github.io/blog/2022-02-07-sbt-tests/</link><pubDate>Mon, 07 Feb 2022 19:53:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2022-02-07-sbt-tests/</guid><description>Últimamente en el trabajo estoy usando mucho delta para algunas tablas de dimensiones y estas tablas realizan actualizaciones parciales de las filas para replicar la lógica de negocio.
Esto, nos lleva a varios tests que replican un estado de la tabla y realizan las actualizaciones pertinentes para comprobar todos los flujos y por ende un sobrecoste de ejecución de ese tipo de tests que acaba siendo agotador.
Una de las soluciones planteadas fue incluir en las builds un parámetro para saltarse el step de ejecución de los tests.</description></item><item><title>Multiplying rows in Spark</title><link>https://adrianabreu.github.io/blog/2021-11-11-multiplying-rows-in-spark/</link><pubDate>Thu, 11 Nov 2021 18:32:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2021-11-11-multiplying-rows-in-spark/</guid><description>Earlier this week I checked on a Pull Request that bothered me since I saw it from the first time. Let&amp;rsquo;s say we work for a bank and we are going to give cash to our clients if they get some people to join our bank.
And we have an advertising campaign definition like this:
campaign_id inviter_cash receiver_cash FakeBank001 50 30 FakeBank002 40 20 FakeBank003 30 20 And then our BI teams defines the schema they want for their dashboards.</description></item><item><title>The horrible azure devops ui</title><link>https://adrianabreu.github.io/blog/2021-11-07-the-horrible-azure-devops-pipeline-ui/</link><pubDate>Sun, 07 Nov 2021 16:32:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2021-11-07-the-horrible-azure-devops-pipeline-ui/</guid><description>Disclaimer: I read the docs, I know this is just complaining and not giving feedback, but man, this UI stills is horrible.
So&amp;hellip; Let&amp;rsquo;s put into situation, there was a connection update between devops and bitbucket and suddenly most of our pipelines stopped working. They told me to change the connection in the yaml file and that didn&amp;rsquo;t work.
I know that there are three parts involving in a pipeline for sure:</description></item><item><title>Regex 101</title><link>https://adrianabreu.github.io/blog/2021-11-01-regex-101/</link><pubDate>Fri, 05 Nov 2021 16:39:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2021-11-01-regex-101/</guid><description>-You will spent your whole life relearning regex, there is a beginning, but never and end.
Last year I participated in some small code problems and practised some regex. I got used to it and feel quite good at it.
And today I had to use it again. I had the following dataframe:
product attributes 1 (SIZE-36) 2 (COLOR-RED) 3 (SIZE-38, COLOR-BLUE) 4 (COLOR-GREEN, SIZE-39) A wonderful set of string merged with properties that could vary.</description></item><item><title>Sbt Intro I</title><link>https://adrianabreu.github.io/blog/2021-10-10-sbt-intro-i/</link><pubDate>Sun, 10 Oct 2021 14:47:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2021-10-10-sbt-intro-i/</guid><description>El mes pasado cambié a otro trabajo :) y por casualidades he vuelto a acabar con proyectos de scala. Este proyecto está bastante avanzado y hace un uso intensivo de los plugins de sbt. De hecho, una tarea que tengo próximamente es hacer una plantilla para proyectos. Así que quería repasar los conceptos básicos de sbt en una serie de posts.
¿Qué es sbt scala build tool? Es una herramienta para gestionar proyectos en scala.</description></item><item><title>A funny bug</title><link>https://adrianabreu.github.io/blog/2021-07-21-funny-bug/</link><pubDate>Wed, 21 Jul 2021 18:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2021-07-21-funny-bug/</guid><description>Ayer mismo estaba intentando analizar unos data_types para completar una exportación al backend. La idea era sencilla, buscar para cada usuario la ultima información disponible en una tabla que representaba actualizaciones sobre su perfil.
Como a su perfil podías &amp;ldquo;añadir&amp;rdquo;, &amp;ldquo;actualizar&amp;rdquo; y &amp;ldquo;eliminar&amp;rdquo; cosas, pues existian los tres tipos en la tabla. Para que la imaginemos mejor sería tal que así:
user_id favorite_stuff operation metadata A Chocolate Add &amp;hellip; A Chocolate Update &amp;hellip; B Milk Remove &amp;hellip; B Cornflakes Add &amp;hellip; De tal manera que habría que combinar todos los eventos para saber cual es el perfil actual del usuario y aplicar cierta lógica.</description></item><item><title>Exportando los datos de firebase</title><link>https://adrianabreu.github.io/blog/2021-05-06-exporting-firebase-data/</link><pubDate>Thu, 06 May 2021 11:49:36 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2021-05-06-exporting-firebase-data/</guid><description>Si trabajamos analizando los datos de una aplicación móvil es muy probable que esté integrado algún sistema para trackear los eventos de la app. Y entre ellos, uno de los más conocidos es Firebase.
Estos eventos contienen mucha información útil y nos permiten por ejemplo saber, un usuario que se ha ido cuanto tiempo ha usado la aplicación o cuantos dias han pasado.
O si realmente ha seguido el flujo de acciones que esperabamos (con un diagrama de sankey podríamos ver donde se han ido los usuarios).</description></item><item><title>Notas sobre storytelling with data</title><link>https://adrianabreu.github.io/blog/2021-01-04-notes-storytelling-with-data-/</link><pubDate>Mon, 04 Jan 2021 19:00:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2021-01-04-notes-storytelling-with-data-/</guid><description>Como uno de los objetivos antes de cambiar de año quería empezar a dar visibilidad sobre el producto en el que estoy trabajando con un dashboard. Tras probar varias opciones, hemos optado por utilizar Quicksight para simplificar los procesos en aws y reducir nuestra infraestructura.
Aún así, empezando un dashboard de cero, es muy difícil transmitir la información de forma clara. Es importante evitar que los usuarios vengan simplemente a expotar sus datos a csv para luego cargarlos en excel.</description></item><item><title>Configurando poetry y gitlab</title><link>https://adrianabreu.github.io/blog/2020-12-30-configurando-poetry-gitlab/</link><pubDate>Wed, 30 Dec 2020 15:42:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-12-30-configurando-poetry-gitlab/</guid><description>Hace poco más de un mes cambié de trabajo y me encontré además con un cambio de stack considerable. Ahora estoy trabajando con aws + github + python. Y bueno, al margen de los cambios de conceptos y demás, me ha llevado bastante encontrar un flujo de trabajo que no me pareciera &amp;ldquo;frágil&amp;rdquo;.
Lo primero y que me ha decepcionado bastante es que github no incluye soporte para hostear paquetes de python.</description></item><item><title>Jugando con Data Factory</title><link>https://adrianabreu.github.io/blog/2020-10-01-jugando-con-df/</link><pubDate>Thu, 01 Oct 2020 10:12:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-10-01-jugando-con-df/</guid><description>Sorprendentemente, hasta ahora, no había tenido la posibilidad de trabajar con data factory, sólo lo habia usado para algunas migraciones de datos.
Sin embargo, tras estabilizar un proyecto y consolidar su nueva etapa, necesitabamos simplificar la solución implementada para migrar datos.
Una representación sencilla de la arquitectura actual sería:
En un flujo muy sencillo sería esto:
La etl escribe un fichero csv con spark en un directorio de un blob storage.</description></item><item><title>Tipos de join en spark</title><link>https://adrianabreu.github.io/blog/2020-12-29-spark-joins/</link><pubDate>Tue, 29 Sep 2020 18:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-12-29-spark-joins/</guid><description>Hace unos días tuve la fortuna (o desgracia) de implementar la lógica más compleja de todo el dominio. El resultado, como esperaba, una etl que falaba por recursos constantementes. El problema:
Caused by: org.apache.spark.SparkException: Could not execute broadcast in 300 secs. You can increase the timeout for broadcasts via spark.sql.broadcastTimeout or disable broadcast join by setting spark.sql.autoBroadcastJoinThreshold to -1 Lo primero fue revisar el plan de ejecución para ver que estaba sucediendo.</description></item><item><title>Calcular el domingo de la semana</title><link>https://adrianabreu.github.io/blog/2020-09-02-calcular-el-domingo-correspondiente/</link><pubDate>Wed, 02 Sep 2020 10:12:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-09-02-calcular-el-domingo-correspondiente/</guid><description>A la hora de publicar reportes es común agrupar los datos por semanas. Otro motivo es alinearse con el negocio donde los cierres pueden producirse en días concretos, por ejemplo, un domingo.
En esos casos si tenemos los datos particionados por días nos interesa saber a que domingo correspondería cada uno de los datos.
Los que venimos de otros entornos tendemos a pensar en esas complicadas librerías de fechas (moment.js, jodatime, etc).</description></item><item><title>Detectando ficheros pequenos Spark</title><link>https://adrianabreu.github.io/blog/2020-08-25-detectando-ficheros-pequenos/</link><pubDate>Tue, 25 Aug 2020 17:22:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-08-25-detectando-ficheros-pequenos/</guid><description>Uno de los mayores problemas de rendimiento que podemos encontrar en los datalake es tener que mover una enorme cantidad de ficheros pequeños, por el overhead que eso representa en las transacciones. Este post de databricks recomendada https://forums.databricks.com/questions/101/what-is-an-optimal-size-for-file-partitions-using.html que se crearan ficheros de 1GB parquet.
Sin embargo mucha gente no sabe como detectar esto. Hace poco estuve jugando con un notebook y usando simplemente las herramientas del dbutils pude clasificar los ficheros que tenia en las entidades del datalake en múltiples categorías, así podría estimar cuantos ficheros había en un rango de tiempo.</description></item><item><title>Spark windows functions (I)</title><link>https://adrianabreu.github.io/blog/2020-08-11-spark-windows-functions/</link><pubDate>Tue, 11 Aug 2020 18:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-08-11-spark-windows-functions/</guid><description>En analítica, es muy común hacer uso de las funciones de ventana para distintos cálculos. Hace poco me encontré con un pequeño problema cuya solución mejoró muchísimo al usar las funciones de ventana, demos un poco de contexto.
Tenemos una dimensión de usuarios donde los usuarios se van registrando con una fecha y tenemos una tabla de ventas donde tenemos las ventas globales para cada día
Y lo que queremos dar es una visión de cómo cada día evoluciona el programa, para ello se quiere que cada día estén tanto las ventas acumuladas como los registros acumulados.</description></item><item><title>Spark windows functions (II)</title><link>https://adrianabreu.github.io/blog/2023-01-01-spark-windows-functions-ii/</link><pubDate>Tue, 11 Aug 2020 18:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2023-01-01-spark-windows-functions-ii/</guid><description>En el post anterior pudimos utilizar las funciones de ventanas para realizar agregados de sumas sobre ventanas temporales. Ahora, me gustaría utilizar otro ejemplo de analítica: Comparar con datos previos.
Pongamos que queremos analizar las ventas de un producto en diversos periodos de tiempo. Es decir, nos interesa saber si ahora vende más o menos que antes.
Para ello partiremos de una tabla de ventas:
DateKey ProductId Sales Ahora que tenemos esto nos interesaría agrupar para cada producto sus ventas</description></item><item><title>Acceso al keyvault mediante certificados</title><link>https://adrianabreu.github.io/blog/2020-05-31-acceso-keyvault-certificados/</link><pubDate>Sat, 30 May 2020 19:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-05-31-acceso-keyvault-certificados/</guid><description>En el proceso de migración de una aplicación de webjob a azure batch, nos encontramos con la problemática de gestionar los secretos. El servicio de batch se encarga de recoger una aplicación de un storage y hacer ejecuciones de ellas (tasks) en unas máquinas concretas (pool).
Para poder gestionar los secretos de la aplicación, estos estaban guardados en keyvault. Y teníamos que acceder de forma segura a ello. Por eso optamos por utilizar la autenticación via certificado.</description></item><item><title>Scala best practices notes</title><link>https://adrianabreu.github.io/blog/2020-04-27-scala-best-practices-nicolas-rinaudo/</link><pubDate>Mon, 27 Apr 2020 20:55:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-04-27-scala-best-practices-nicolas-rinaudo/</guid><description>He aprovechado estos días de cuarentena para revisar algunos de los &amp;ldquo;huecos&amp;rdquo; de conocimiento que tenía en Scala. Una de las charlas que he podido ver es esta: Scala best practices I wish someone&amp;rsquo;d told me about - Nicolas Rinaudo
Por supuesto siempre recomiendo ver la charla, pero he querido condensar (aún más) ese conocimiento en este post, insisto, es amena y muy interesante, muchos de los puntos que se definen en la charla no se han explicado porque la mayoría se resuelven en dotty y aunque</description></item><item><title>Notas sobre programación funcional en Scala I</title><link>https://adrianabreu.github.io/blog/2020-04-07-notes-on-functional-programming-in-scala-i/</link><pubDate>Mon, 06 Apr 2020 18:22:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-04-07-notes-on-functional-programming-in-scala-i/</guid><description>Hace unos días pude comprarme el libro de Paul Chiusano y Rúnar Bjarnason: Functional Programming in scala y no puedo estar más contento con él.
Como ya es costumbre, aprovecho para dejar mis notas sobre el libro en el blog. No se trata de un resumen del mismo sino curiosidades que sé que volveré a consultar en un futuro. Intentaré que no queden post excesivamente largos haciendo un por capítulo. Igualmente, recomiendo a todo el mundo adquirir &amp;ldquo;el libro rojo de Scala&amp;rdquo; y echarle un vistazo.</description></item><item><title>Límites en azure functions para procesos de larga duración</title><link>https://adrianabreu.github.io/blog/2020-02-24-azure-functions-service-bus-limits/</link><pubDate>Mon, 24 Feb 2020 19:22:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-02-24-azure-functions-service-bus-limits/</guid><description>Estas últimas semanas he tenido que implementar ciertas mejoras en un proyecto. El objetivo era muy simple, conectar el proyecto a una aplicación de datawarehousing existente, y de forma externa, realizar agregados y luego aplicar cierto procesamiento para un servicio en particular.
Además había una serie de requisitos extras:
El procesamiento iba a ser reutilizado por otro proyecto. Y requería comprimir y cifrar archivos grandes. La primera parte tenía que simplemente, Había una deadline muy cercana para este proyecto.</description></item><item><title>Conceptos básicos de Spark</title><link>https://adrianabreu.github.io/blog/2019-11-09-spark-concepts-basicos/</link><pubDate>Sat, 09 Nov 2019 19:22:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2019-11-09-spark-concepts-basicos/</guid><description>Nota del autor: Todos los contenidos de este artículo son extractos del libro &amp;ldquo;The Data Engineer&amp;rsquo;s Guide to Apache Spark&amp;rdquo; que puedes descargar desde la pagina de databricks: https://databricks.com/lp/ebook/data-engineer-spark-guide
Preludio: Cluster: Un cluster no es más que un conjunto de máquinas trabajando de forma coordinada. Un cluster de Spark se compone de nodos. Uno actúa como DRIVER y es el punto de entrada para el código del usuario. Los otros actúan como EXECUTOR que seran los encargados de realizar las operaciones.</description></item><item><title>Empezando en Spark con Docker</title><link>https://adrianabreu.github.io/blog/2019-11-10-empezando-en-spark-con-docker/</link><pubDate>Sat, 09 Nov 2019 19:22:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2019-11-10-empezando-en-spark-con-docker/</guid><description>A pesar de haber leído guías tan buenas como:
https://medium.com/@bogdan.cojocar/how-to-run-scala-and-spark-in-the-jupyter-notebook-328a80090b3b
https://medium.com/@singhpraveen2010/install-apache-spark-and-configure-with-jupyter-notebook-in-10-minutes-ae120ebca597
Se me ha hecho cuesta arriba el poder conectar un notebook de jupyter y utilizar Scala. Entre configurar el apache toree para poder usar scala en los notebooks y algún error luego en spark al usarlo desde IntelliJ, me he acabado rindiendo.
Nota del autor: Como disclaimer esto ocurre probablemente porque estoy en Manjaro y mi version de Scala es incompatible.</description></item><item><title>Correlated subqueries</title><link>https://adrianabreu.github.io/blog/2019-09-26-correlated-subqueries/</link><pubDate>Thu, 26 Sep 2019 20:43:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2019-09-26-correlated-subqueries/</guid><description>Llevo un par de meses viendo como la mayoría de esfuerzos en el proyecto en el que estoy se centran en evitar los joins en las distintas capas de análisis. Aprovechando las capacidades de spark se busca tener las estructuras muy desnormalizadas y se había &amp;ldquo;endemoniado&amp;rdquo; al join considerarlo perjudicial.
Tanto es así que llevo un par de días peleando con una pieza de código que me ha sorprendido. Partiendo de una tabla de hechos que agrupa datos para un periodo a hasta b, se quiere que se &amp;ldquo;colapsen&amp;rdquo; los datos de hace 14 días.</description></item><item><title>Depurando queries en hive</title><link>https://adrianabreu.github.io/blog/2023-05-17-depurando-hive-queries/</link><pubDate>Fri, 17 May 2019 22:43:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2023-05-17-depurando-hive-queries/</guid><description>En la última he podido ver qué pasa cuando el volumen de datos aumenta de forma exponencial, una query que funcionaba en un tiempo razonable para un volumen considerable ha empezado a fallar a pesar de los reintentos.
¿El origen?
Status: Failed 19/05/21 01:05:30 [main]: ERROR SessionState: Status: Failed Vertex failed, vertexName=Map 10, vertexId=vertex_1558386441915_0035_1_01, diagnostics=[Task failed, taskId=task_1558386441915_0035_1_01_000002, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.OutOfMemoryError: Java heap space Esto se traduce en que uno de los vértices, ha tenido que mover demasiados datos y hemos acabado sin memoria.</description></item><item><title>Datos I - Introducción al Datawarehousing</title><link>https://adrianabreu.github.io/blog/2019-02-05-data-i-introduccion-al-datawarehouse/</link><pubDate>Tue, 05 Feb 2019 14:43:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2019-02-05-data-i-introduccion-al-datawarehouse/</guid><description>En los últimos meses mi trabajo ha pivotado del mundo de la web al mundo de los datos. He entrado a participar en un proyecto de Data Warehouse y he acabado muy contento en él. Hace unos días mi cambio se oficializó completamente y ahora me he dado cuenta de que no solo tengo un mundo técnico ante mí, sino que además necesito consolidar algunas bases teóricas.
Investigando la bibliografía, me han recomendado en Reddit: The Data Warehouse Toolkit, The Complete Guide to Dimensional Modeling 2nd Edition.</description></item><item><title>Angular Series III - Dynamic components</title><link>https://adrianabreu.github.io/blog/2017-12-17-angular-series-iii-dynamic-components/</link><pubDate>Sun, 17 Dec 2017 14:43:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2017-12-17-angular-series-iii-dynamic-components/</guid><description>Antes de terminar repasando el tema de templating, quiero hacer un inciso. Existen ciertos casos donde el templating es insuficiente y lo que necesitamos es simplemente escoger dinámicamente que componente vamos a renderizar.
Esto está documentado en la documentación de angular bajo el nombre de Dynamic Components.
¿Cómo funcionan estos dynamics components? Explicado mal y pronto, la idea es: Escoger un elemento de la vista que actue de contenedor e inyectar el componente debe ir ahí.</description></item><item><title>Angular Series II - Templating</title><link>https://adrianabreu.github.io/blog/2017-11-18-angular-series-ii-templating/</link><pubDate>Sat, 18 Nov 2017 18:53:17 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2017-11-18-angular-series-ii-templating/</guid><description>Continuando con el artículo del otro día sobre proyección de contenido aquí pretendo mostrar otra forma de pasar contenido: las templates. ¿Qué es una template? Es un pedazo de html envuelto entre entre etiquetas ng-template tal que así:
&amp;lt;ng-template&amp;gt; &amp;lt;div class=”as-template”&amp;gt; I won’t be rendered &amp;lt;/div&amp;gt; &amp;lt;/ng-template&amp;gt; Si esto lo ponemos en un componente, tal como en el siguiente ejemplo en el navegador aparecerá: NADA.
¿Por qué? Porque las plantillas de angular no se renderizan al ser evaluadas.</description></item><item><title>Experimentando con Redux</title><link>https://adrianabreu.github.io/blog/2017-11-06-experimentando-con-redux/</link><pubDate>Mon, 06 Nov 2017 23:18:17 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2017-11-06-experimentando-con-redux/</guid><description>Con el objetivo de mejorar el simulador SIMDE me decidí a probar un sistema de gestión de estados y concentrar el funcionamiento de la aplicación. ¿El motivo? Era la única forma sensata que tenía de poder gestionar la UI cuando entrara la máquina VLIW sin que todo fuera un caos.
Para ello he recurrido a mi aplicación favorita: Chuck Norris client app. (Ya la he hecho en AngularJS y Angular previamente).</description></item><item><title>Un breve sumario</title><link>https://adrianabreu.github.io/blog/2017-10-08-breve-sumario/</link><pubDate>Sun, 08 Oct 2017 18:28:17 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2017-10-08-breve-sumario/</guid><description>El mes de septiembre ha sido un poco caótico, pero ya creo que me he adaptado a la rutina. Por lo pronto he empezado con el blog, ahora el auto despliegue va mucho mejor, estoy utilizando CircleCI en vez de Wecker.
Por otra parte, el diseño del blog se ha separado por mucho del tema original, y no contento con esto he mejorado el desarrollo del tema permitiendo el uso de Sass.</description></item><item><title>Angular series I - Proyección de contenido (Content projection)</title><link>https://adrianabreu.github.io/blog/2017-08-14-angular-series-i-transclusion/</link><pubDate>Mon, 14 Aug 2017 17:32:37 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2017-08-14-angular-series-i-transclusion/</guid><description>Cuando creé este blog tenía bastante claro que uno de los objetivos principales era que me sirviera de utilidad para interiorizar lo que voy aprendiendo. Y aunque he escrito ciertas cosas útiles para mi día a día no estoy registrando ni una centésima parte de la información que mi mente ha ido procesando estos meses.
Así que voy a dedicarme a escribir un artículo semanal sobre un tema con el que llevo ya casi un año: Angular.</description></item><item><title>Auto deployment en gh-pages con Travis</title><link>https://adrianabreu.github.io/blog/2017-08-04-autodeploy-con-travis/</link><pubDate>Fri, 04 Aug 2017 16:37:11 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2017-08-04-autodeploy-con-travis/</guid><description>Una de las mejores características de github sin duda alguna, son las gh-pages.
Las gh-pages nos permiten desplegar el código de nuestra aplicación frontend a través de esta rama, de tal forma que muchos de nuestros proyectos (por ejemplo este blog) estén disponibles sin tener limitaciones de hosting.
Pero sin duda una desventaja es el hecho de tener que mantener el deploy de nuestras revisiones: cambiar de rama, eliminar el contenido, hacer una build y desplegar.</description></item><item><title>Mi experiencia con React</title><link>https://adrianabreu.github.io/blog/2017-07-28-mi-experiencia-con-react/</link><pubDate>Fri, 28 Jul 2017 17:11:10 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2017-07-28-mi-experiencia-con-react/</guid><description>Hoy se cumple un mes desde que entregué mi trabajo de fin de grado. (Modestia aparte, conseguí un 10). Han pasado muchas cosas desde entonces, como que por ejemplo ahora mismo estoy viviendo en Barcelona y que trabajo para nada más y nada menos que Plain Concepts. Pero al margen de eso, vamos a centrarnos en mi trabajo de fin de grado.
En tercero de grado de ingeniería informática es posible escoger una especialidad.</description></item><item><title>Empezar a programar</title><link>https://adrianabreu.github.io/blog/2017-06-04-empezar-a-programar/</link><pubDate>Sun, 04 Jun 2017 13:07:30 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2017-06-04-empezar-a-programar/</guid><description>Creo que me he visto en más de una decena de ocasiones aconsejando a gente como empezar en el mundo de la programación, así que esta entrada viene dedicada a todos ellos.
Por supuesto se trata de una guía basada en mi opinión y solo mi opinión.
¿DAW o DAM? Realmente mi respuesta es bastante clara: DAW.
El mundo web cada vez va tomando más y más fuerza. Ya se pueden desarrollar aplicaciones de escritorio utilizando como base aplicaciones web con tecnologías como electron (un buen ejemplo de este tipo de aplicaciones Visual Studio Code).</description></item><item><title>Docker</title><link>https://adrianabreu.github.io/blog/2017-04-09-docker/</link><pubDate>Sun, 09 Apr 2017 15:20:44 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2017-04-09-docker/</guid><description>¿Qué es docker? Docker es un software que permite automatizar el despliegue de aplicaciones utilizando contenedores.
Los contenedores hacen uso de la virtualización a nivel de sistema operativo, con lo cual son mucho más ligeros y rápidos que las máquinas virtuales.
¿Para qué lo puedo necesitar? En el día a día, una de las cosas para las que Docker me resulta más útil sin duda es para desplegar las bases de datos en desarrollo.</description></item><item><title>Capas en el backend</title><link>https://adrianabreu.github.io/blog/2017-04-08-capas-en-el-backend/</link><pubDate>Sat, 08 Apr 2017 17:33:10 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2017-04-08-capas-en-el-backend/</guid><description>Una de las cosas más difíciles cuando ves un proyecto de backend por primera vez es discernir la funcionalidad y responsabilidad de las distintas capas. Así que me he planteado dar una visión general.
Antes de nada, quiero aclarar que es probable que esta nomenclatura no coincida con otra que veais por ahí, existen muchos sinónimos para los mismos conceptos, yo por mi parte, voy a explicar cual utilizo actualmente.</description></item><item><title>Interacción a través de ViewChild en Angular</title><link>https://adrianabreu.github.io/blog/2017-03-24-interaccion-a-traves-de-viewchild/</link><pubDate>Fri, 24 Mar 2017 23:11:22 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2017-03-24-interaccion-a-traves-de-viewchild/</guid><description>Hoy me he encontrado con una situación peculiar en un código. Aunque considero que quizás como está realizada la tarea no sea la mejor opción, creo que es un buen ejemplo para entender algunos conceptos de Angular.
Partiendo de una aplicaicón muy básica donde tenemos dos componentes: AppComponent y ChildComponent, vamos a renderizar dinámicamente el componente hijo desde el componente padre y ejecutar una serie de acciones.
Empecemos por el componente padre:</description></item><item><title>Como construir un portfolio</title><link>https://adrianabreu.github.io/blog/2017-03-12-como-construir-un-portfolio/</link><pubDate>Sun, 12 Mar 2017 10:39:22 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2017-03-12-como-construir-un-portfolio/</guid><description>Desde hace un tiempo en los países anglosajones los desarrolladores tienen una herramienta más importante que su CV, el portfolio.
Un portfolio no es más que una muestra de tus trabajos y una justificación de las habilidades escritas en tu CV. A día de hoy, es increíblemente fácil tener un portfolio. Pero ya que voy a hablar de eso, aprovecharé para hablar también de qué debería contener un portfolio, y como enfocar el portfolio de un desarrollador junior proporcionando algunas ideas básicas.</description></item><item><title>Sin componentes a componentes</title><link>https://adrianabreu.github.io/blog/2017-03-11-sin-componentes-a-componentes/</link><pubDate>Sat, 11 Mar 2017 17:44:11 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2017-03-11-sin-componentes-a-componentes/</guid><description>Hace unos meses mientras me formaba en Angular 1 hice con un amigo un cliente para una api que proporcionaba frases de Chuck Norris.
Fue un desarrollo divertido donde almacenábamos las frases descargadas en localStorage y permítiamos filtrar las frases por categorías.
El diseño era modular y me quedé contento con lo que aprendí. Pero entonces entré en prácticas en la empresa y me puse a formarme en Angular 2.</description></item><item><title>Quizás deberías empezar un blog</title><link>https://adrianabreu.github.io/blog/2017-03-04-quizas-deberias-empezara-un-blog/</link><pubDate>Sat, 04 Mar 2017 19:46:22 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2017-03-04-quizas-deberias-empezara-un-blog/</guid><description>Me encanta leer artículos de programación y de tecnología. Leo muchísimos artículos.
Es más, considero que paso al menos de media, 2 horas diarias leyendo esta clase de artículos.
Pero soy otro lector invisible más, no devuelvo lo que aprendo a la comunidad (aunque ahora me he ido animando con los comentarios) y lo peor, no lo almaceno.
Un blog puede ser una parte más de tu portfolio, puede recoger tus dudas y tu avance.</description></item><item><title>Probando Hugo</title><link>https://adrianabreu.github.io/blog/2017-02-05-probando-hugo/</link><pubDate>Sun, 05 Feb 2017 17:31:42 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2017-02-05-probando-hugo/</guid><description>La verdad es que estoy asombrado. Hexo me tenía muy contento, pero aún así el rendimiento no me convencía tanto como esperaba. Y me pasaba los días dando largas hasta ponerme a postear. Aprovechando que quería hacer una pequeña limpieza antes de ponerme con el trabajo de fin de grado, he estado mirando este generador, Hugo.
Este generador basado en el lenguaje Go, es increíblemente potente, portable, y sencillo de instalar.</description></item><item><title>Nuevos comienzos</title><link>https://adrianabreu.github.io/blog/2017-01-08-nuevos-comienzos/</link><pubDate>Sun, 08 Jan 2017 10:22:36 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2017-01-08-nuevos-comienzos/</guid><description>No creía que fuera a estar escribiendo esto tan pronto, pero hace poco terminé de realizar las prácticas externas y ya tengo un trabajo en esa misma empresa.
Mis labores, al igual que en las prácticas, consiste en desarrollar utilizando Angular 2 y Java.
He ido aprendiendo (y sigo, y seguiré) muchísimo. Y es por eso que necesito este blog. Un lugar donde compartir toda la información que voy absorbiendo sobre esta tecnología que cada día me gusta más.</description></item></channel></rss>